Epoch: 1, Batch: 1, Loss: 3.3888754844665527
Epoch: 1, Batch: 2, Loss: 1.7544312477111816
Epoch: 1, Batch: 3, Loss: 1.9210518598556519
Epoch: 1, Batch: 4, Loss: 0.9263181686401367
Epoch: 1, Batch: 5, Loss: 0.6756220459938049
Epoch: 1, Batch: 6, Loss: 1.0479789972305298
Epoch: 1, Batch: 7, Loss: 0.9348665475845337
Epoch: 1, Batch: 8, Loss: 0.8983843922615051
Epoch: 1, Batch: 9, Loss: 0.7680216431617737
Epoch: 1, Batch: 10, Loss: 0.5730628371238708
Epoch: 1, Batch: 11, Loss: 0.7596573829650879
Epoch: 1, Batch: 12, Loss: 0.7419312596321106
Epoch: 1, Batch: 13, Loss: 0.4742605686187744
Epoch: 1, Batch: 14, Loss: 0.5063584446907043
Epoch: 1, Batch: 15, Loss: 0.7073203325271606
Epoch: 1, Batch: 16, Loss: 0.608304500579834
Epoch: 1, Batch: 17, Loss: 2.4039273262023926
Epoch: 1, Batch: 18, Loss: 0.56512451171875
Epoch: 1, Batch: 19, Loss: 0.707901120185852
Epoch: 1, Batch: 20, Loss: 0.9914617538452148
Epoch: 1, Batch: 21, Loss: 0.6333380341529846
Epoch: 1, Batch: 22, Loss: 1.152971625328064
Epoch: 1, Batch: 23, Loss: 0.6553768515586853
Epoch: 1, Batch: 24, Loss: 0.7921350002288818
Epoch: 1, Batch: 25, Loss: 0.9094274044036865
Epoch: 1, Batch: 26, Loss: 0.5813952684402466
Epoch: 1, Batch: 27, Loss: 0.5341825485229492
Epoch: 1, Batch: 28, Loss: 0.7717331647872925
Epoch: 1, Batch: 29, Loss: 0.8320428133010864
Epoch: 1, Batch: 30, Loss: 0.7247918248176575
Epoch: 1, Batch: 31, Loss: 1.9402737617492676
Epoch: 1, Batch: 32, Loss: 0.7977869510650635
Epoch: 1, Batch: 33, Loss: 0.8429538011550903
Epoch: 1, Batch: 34, Loss: 0.9924627542495728
Epoch: 1, Batch: 35, Loss: 0.45594191551208496
Epoch: 1, Batch: 36, Loss: 0.43832334876060486
Epoch: 1, Batch: 37, Loss: 0.9202697277069092
Epoch: 1, Batch: 38, Loss: 0.3782941401004791
Epoch: 1, Batch: 39, Loss: 0.3356966972351074
Epoch: 1, Batch: 40, Loss: 1.26043701171875
Epoch: 1, Batch: 41, Loss: 0.4852488040924072
Epoch: 1, Batch: 42, Loss: 0.6572081446647644
Epoch: 1, Batch: 43, Loss: 0.4723116159439087
Epoch: 1, Batch: 44, Loss: 0.67183518409729
Epoch: 1, Batch: 45, Loss: 0.6593102216720581
Epoch: 1, Batch: 46, Loss: 0.6334162354469299
Epoch: 1, Batch: 47, Loss: 0.47133174538612366
Epoch: 1, Batch: 48, Loss: 0.5305626392364502
Epoch: 1, Batch: 49, Loss: 0.7639471888542175
Epoch: 1, Batch: 50, Loss: 0.5952563285827637
Epoch: 1, Batch: 51, Loss: 0.7466225028038025
Epoch: 1, Batch: 52, Loss: 1.5654946565628052
Epoch: 1, Batch: 53, Loss: 0.9034976959228516
Epoch: 1, Batch: 54, Loss: 0.5409240126609802
Epoch: 1, Batch: 55, Loss: 0.8151784539222717
Epoch: 1, Batch: 56, Loss: 0.8508952856063843
Epoch: 1, Batch: 57, Loss: 0.8046087026596069
Epoch: 1, Batch: 58, Loss: 0.7519787549972534
Epoch: 1, Batch: 59, Loss: 0.6195932626724243
Epoch: 1, Batch: 60, Loss: 0.8279210925102234
Epoch: 1, Batch: 61, Loss: 0.5404797792434692
Epoch: 1, Batch: 62, Loss: 0.7504523396492004
Epoch: 1, Batch: 63, Loss: 0.7544803619384766
Epoch: 1, Batch: 64, Loss: 0.5846788883209229
Epoch: 1, Batch: 65, Loss: 0.6126875281333923
Epoch: 1, Batch: 66, Loss: 0.5270952582359314
Epoch: 1, Batch: 67, Loss: 0.9870651364326477
Epoch: 1, Batch: 68, Loss: 0.8360812664031982
Epoch: 1, Batch: 69, Loss: 0.7582271695137024
Epoch: 1, Batch: 70, Loss: 0.9066635370254517
Epoch: 1, Batch: 71, Loss: 0.8034898042678833
Epoch: 1, Batch: 72, Loss: 0.9499732851982117
Epoch: 1, Batch: 73, Loss: 0.6807507872581482
Epoch: 1, Batch: 74, Loss: 0.5507234930992126
Epoch: 1, Batch: 75, Loss: 0.5176113247871399
Epoch: 1, Batch: 76, Loss: 0.691081702709198
Epoch: 1, Batch: 77, Loss: 0.4142395257949829
Epoch: 1, Batch: 78, Loss: 1.6133559942245483
Epoch: 1, Batch: 79, Loss: 0.38099661469459534
Epoch: 1, Batch: 80, Loss: 0.4464503228664398
Epoch: 1, Batch: 81, Loss: 0.885364294052124
Epoch: 1, Batch: 82, Loss: 0.43587005138397217
Epoch: 1, Batch: 83, Loss: 0.47710371017456055
Epoch: 1, Batch: 84, Loss: 0.9930763244628906
Epoch: 1, Batch: 85, Loss: 0.9913569092750549
Epoch: 1, Batch: 86, Loss: 1.620824933052063
Epoch: 1, Batch: 87, Loss: 0.3699464201927185
Epoch: 1, Batch: 88, Loss: 0.5039113759994507
Epoch: 1, Batch: 89, Loss: 0.5020185708999634
Epoch: 1, Batch: 90, Loss: 0.6931756734848022
Epoch: 1, Batch: 91, Loss: 0.6469268798828125
Epoch: 1, Batch: 92, Loss: 0.7418107390403748
Epoch: 1, Batch: 93, Loss: 0.5846228003501892
Epoch: 1, Batch: 94, Loss: 0.45757919549942017
Epoch: 1, Batch: 95, Loss: 0.7523384690284729
Epoch: 1, Batch: 96, Loss: 0.4356417655944824
Epoch: 1, Batch: 97, Loss: 0.42708009481430054
Epoch: 1, Batch: 98, Loss: 0.7987947463989258
Epoch: 1, Batch: 99, Loss: 0.8105605840682983
Epoch: 1, Batch: 100, Loss: 0.8222757577896118
Epoch: 1, Batch: 101, Loss: 0.48390015959739685
Epoch: 1, Batch: 102, Loss: 1.2517298460006714
Epoch: 1, Batch: 103, Loss: 0.6613448858261108
Epoch: 1, Batch: 104, Loss: 0.45943590998649597
Epoch: 1, Batch: 105, Loss: 0.48985791206359863
Epoch: 1, Batch: 106, Loss: 0.5932050347328186
Epoch: 1, Batch: 107, Loss: 1.0268758535385132
Epoch: 1, Batch: 108, Loss: 0.5238457918167114
Epoch: 1, Batch: 109, Loss: 0.7302250862121582
Epoch: 1, Batch: 110, Loss: 0.459656298160553
Epoch: 1, Batch: 111, Loss: 0.6043632626533508
Epoch: 1, Batch: 112, Loss: 0.4741763174533844
Epoch: 1, Batch: 113, Loss: 0.4093252718448639
Epoch: 1, Batch: 114, Loss: 0.2438463568687439
Epoch: 1, Batch: 115, Loss: 0.31263652443885803
Epoch: 1, Batch: 116, Loss: 0.3435231149196625
Epoch: 1, Batch: 117, Loss: 0.4433085322380066
Epoch: 1, Batch: 118, Loss: 0.3088585436344147
Epoch: 1, Batch: 119, Loss: 0.5244617462158203
Epoch: 1, Batch: 120, Loss: 0.34751367568969727
Epoch: 1, Batch: 121, Loss: 0.9215279221534729
Epoch: 1, Batch: 122, Loss: 0.5663278102874756
Epoch: 1, Batch: 123, Loss: 0.8790082335472107
Epoch: 1, Batch: 124, Loss: 0.33358439803123474
Epoch: 1, Batch: 125, Loss: 0.5933711528778076
Epoch: 1, Batch: 126, Loss: 1.0612866878509521
Epoch: 1, Batch: 127, Loss: 0.8176661729812622
Epoch: 1, Batch: 128, Loss: 1.0187690258026123
Epoch: 1, Batch: 129, Loss: 0.7125492095947266
Epoch: 1, Batch: 130, Loss: 0.9874070882797241
Epoch: 1, Batch: 131, Loss: 0.6542935967445374
Epoch: 1, Batch: 132, Loss: 0.40064871311187744
Epoch: 1, Batch: 133, Loss: 0.7970654964447021
Epoch: 1, Batch: 134, Loss: 0.5847839713096619
Epoch: 1, Batch: 135, Loss: 0.33374080061912537
Epoch: 1, Batch: 136, Loss: 0.8313658833503723
Epoch: 1, Batch: 137, Loss: 0.4815424382686615
Epoch: 1, Batch: 138, Loss: 0.342072993516922
Epoch: 1, Batch: 139, Loss: 0.4396611750125885
Epoch: 1, Batch: 140, Loss: 0.28449544310569763
Epoch: 1, Batch: 141, Loss: 0.28795239329338074
Epoch: 1, Batch: 142, Loss: 0.27812469005584717
Epoch: 1, Batch: 143, Loss: 0.28802675008773804
Epoch: 1, Batch: 144, Loss: 0.4194176495075226
Epoch: 1, Batch: 145, Loss: 1.7664110660552979
Epoch: 1, Batch: 146, Loss: 0.5313644409179688
Epoch: 1, Batch: 147, Loss: 0.5638559460639954
Epoch: 1, Batch: 148, Loss: 0.7938662767410278
Epoch: 1, Batch: 149, Loss: 0.7347509860992432
Epoch: 1, Batch: 150, Loss: 0.901293158531189
Epoch: 1, Batch: 151, Loss: 0.6816613674163818
Epoch: 1, Batch: 152, Loss: 0.5196030139923096
Epoch: 1, Batch: 153, Loss: 0.7846558094024658
Epoch: 1, Batch: 154, Loss: 1.0605435371398926
Epoch: 1, Batch: 155, Loss: 0.8819847702980042
Epoch Completed: 1/10, Time: 209.19529366493225, Train Loss: 0.7364869110045894, Valid Loss: 0.6461798954394555

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:15  model_time: 0.3304 (0.3304)  evaluator_time: 0.0086 (0.0086)  time: 0.3780  data: 0.0358  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3280 (0.3236)  evaluator_time: 0.0063 (0.0072)  time: 0.3626  data: 0.0341  max mem: 4996
Test: Total time: 0:00:14 (0.3693 s / it)
Averaged stats: model_time: 0.3280 (0.3236)  evaluator_time: 0.0063 (0.0072)
Accumulating evaluation results...
DONE (t=0.07s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.057
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.058
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.090
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.247
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.417
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.252
Epoch: 2, Batch: 156, Loss: 0.8879193067550659
Epoch: 2, Batch: 157, Loss: 0.7687078714370728
Epoch: 2, Batch: 158, Loss: 0.8723046183586121
Epoch: 2, Batch: 159, Loss: 0.5750688910484314
Epoch: 2, Batch: 160, Loss: 0.40385866165161133
Epoch: 2, Batch: 161, Loss: 0.4590911865234375
Epoch: 2, Batch: 162, Loss: 0.678170919418335
Epoch: 2, Batch: 163, Loss: 0.4256272614002228
Epoch: 2, Batch: 164, Loss: 0.48389172554016113
Epoch: 2, Batch: 165, Loss: 0.48644590377807617
Epoch: 2, Batch: 166, Loss: 0.4718886613845825
Epoch: 2, Batch: 167, Loss: 0.3273097574710846
Epoch: 2, Batch: 168, Loss: 0.28466737270355225
Epoch: 2, Batch: 169, Loss: 0.4497109055519104
Epoch: 2, Batch: 170, Loss: 0.38415515422821045
Epoch: 2, Batch: 171, Loss: 0.2575894296169281
Epoch: 2, Batch: 172, Loss: 2.0312066078186035
Epoch: 2, Batch: 173, Loss: 0.34764865040779114
Epoch: 2, Batch: 174, Loss: 0.44599223136901855
Epoch: 2, Batch: 175, Loss: 0.956473708152771
Epoch: 2, Batch: 176, Loss: 0.5765423774719238
Epoch: 2, Batch: 177, Loss: 1.014541506767273
Epoch: 2, Batch: 178, Loss: 0.4645709991455078
Epoch: 2, Batch: 179, Loss: 0.7014712691307068
Epoch: 2, Batch: 180, Loss: 0.6558770537376404
Epoch: 2, Batch: 181, Loss: 0.4454241991043091
Epoch: 2, Batch: 182, Loss: 0.43721848726272583
Epoch: 2, Batch: 183, Loss: 0.72148597240448
Epoch: 2, Batch: 184, Loss: 0.6503321528434753
Epoch: 2, Batch: 185, Loss: 0.6570526957511902
Epoch: 2, Batch: 186, Loss: 2.070155382156372
Epoch: 2, Batch: 187, Loss: 0.7731021642684937
Epoch: 2, Batch: 188, Loss: 0.6027644872665405
Epoch: 2, Batch: 189, Loss: 0.9174225330352783
Epoch: 2, Batch: 190, Loss: 0.37314632534980774
Epoch: 2, Batch: 191, Loss: 0.43405207991600037
Epoch: 2, Batch: 192, Loss: 0.8047782182693481
Epoch: 2, Batch: 193, Loss: 0.47095438838005066
Epoch: 2, Batch: 194, Loss: 0.3109814524650574
Epoch: 2, Batch: 195, Loss: 0.8764655590057373
Epoch: 2, Batch: 196, Loss: 0.4307837188243866
Epoch: 2, Batch: 197, Loss: 0.5212692022323608
Epoch: 2, Batch: 198, Loss: 0.478089839220047
Epoch: 2, Batch: 199, Loss: 0.5534309148788452
Epoch: 2, Batch: 200, Loss: 0.5176834464073181
Epoch: 2, Batch: 201, Loss: 0.45741787552833557
Epoch: 2, Batch: 202, Loss: 0.376533180475235
Epoch: 2, Batch: 203, Loss: 0.40995508432388306
Epoch: 2, Batch: 204, Loss: 0.5977603197097778
Epoch: 2, Batch: 205, Loss: 0.6280345320701599
Epoch: 2, Batch: 206, Loss: 0.49787604808807373
Epoch: 2, Batch: 207, Loss: 1.0180991888046265
Epoch: 2, Batch: 208, Loss: 0.7786237001419067
Epoch: 2, Batch: 209, Loss: 0.4536243677139282
Epoch: 2, Batch: 210, Loss: 0.49483534693717957
Epoch: 2, Batch: 211, Loss: 0.6827256679534912
Epoch: 2, Batch: 212, Loss: 0.5953354239463806
Epoch: 2, Batch: 213, Loss: 0.49107423424720764
Epoch: 2, Batch: 214, Loss: 0.4490911364555359
Epoch: 2, Batch: 215, Loss: 0.49823832511901855
Epoch: 2, Batch: 216, Loss: 0.3758768141269684
Epoch: 2, Batch: 217, Loss: 0.49220576882362366
Epoch: 2, Batch: 218, Loss: 0.5863745808601379
Epoch: 2, Batch: 219, Loss: 0.4591560363769531
Epoch: 2, Batch: 220, Loss: 0.4800850749015808
Epoch: 2, Batch: 221, Loss: 0.38220757246017456
Epoch: 2, Batch: 222, Loss: 0.7609055042266846
Epoch: 2, Batch: 223, Loss: 0.7241405248641968
Epoch: 2, Batch: 224, Loss: 0.7206801176071167
Epoch: 2, Batch: 225, Loss: 0.8569880127906799
Epoch: 2, Batch: 226, Loss: 0.6460300087928772
Epoch: 2, Batch: 227, Loss: 0.81060791015625
Epoch: 2, Batch: 228, Loss: 0.5834842324256897
Epoch: 2, Batch: 229, Loss: 0.42697805166244507
Epoch: 2, Batch: 230, Loss: 0.42596691846847534
Epoch: 2, Batch: 231, Loss: 0.5635477304458618
Epoch: 2, Batch: 232, Loss: 0.3161138892173767
Epoch: 2, Batch: 233, Loss: 1.3936212062835693
Epoch: 2, Batch: 234, Loss: 0.3377421498298645
Epoch: 2, Batch: 235, Loss: 0.3323151767253876
Epoch: 2, Batch: 236, Loss: 0.827538251876831
Epoch: 2, Batch: 237, Loss: 0.3405892252922058
Epoch: 2, Batch: 238, Loss: 0.38492313027381897
Epoch: 2, Batch: 239, Loss: 1.150456190109253
Epoch: 2, Batch: 240, Loss: 1.097427487373352
Epoch: 2, Batch: 241, Loss: 1.4547594785690308
Epoch: 2, Batch: 242, Loss: 0.3457421660423279
Epoch: 2, Batch: 243, Loss: 0.5238518714904785
Epoch: 2, Batch: 244, Loss: 0.46767544746398926
Epoch: 2, Batch: 245, Loss: 0.601387619972229
Epoch: 2, Batch: 246, Loss: 0.5424701571464539
Epoch: 2, Batch: 247, Loss: 0.5863111019134521
Epoch: 2, Batch: 248, Loss: 0.4963189959526062
Epoch: 2, Batch: 249, Loss: 0.35653573274612427
Epoch: 2, Batch: 250, Loss: 0.7672266960144043
Epoch: 2, Batch: 251, Loss: 0.36606815457344055
Epoch: 2, Batch: 252, Loss: 0.3928092420101166
Epoch: 2, Batch: 253, Loss: 0.7797166109085083
Epoch: 2, Batch: 254, Loss: 0.7239821553230286
Epoch: 2, Batch: 255, Loss: 0.8860143423080444
Epoch: 2, Batch: 256, Loss: 0.4045366644859314
Epoch: 2, Batch: 257, Loss: 1.3121154308319092
Epoch: 2, Batch: 258, Loss: 0.5878544449806213
Epoch: 2, Batch: 259, Loss: 0.46065253019332886
Epoch: 2, Batch: 260, Loss: 0.4522596299648285
Epoch: 2, Batch: 261, Loss: 0.5055087804794312
Epoch: 2, Batch: 262, Loss: 0.9748942852020264
Epoch: 2, Batch: 263, Loss: 0.4421583116054535
Epoch: 2, Batch: 264, Loss: 0.546709418296814
Epoch: 2, Batch: 265, Loss: 0.3968088924884796
Epoch: 2, Batch: 266, Loss: 0.4325709044933319
Epoch: 2, Batch: 267, Loss: 0.4243919551372528
Epoch: 2, Batch: 268, Loss: 0.40392249822616577
Epoch: 2, Batch: 269, Loss: 0.221285879611969
Epoch: 2, Batch: 270, Loss: 0.2829284369945526
Epoch: 2, Batch: 271, Loss: 0.31388410925865173
Epoch: 2, Batch: 272, Loss: 0.4126521944999695
Epoch: 2, Batch: 273, Loss: 0.3141569197177887
Epoch: 2, Batch: 274, Loss: 0.4949946403503418
Epoch: 2, Batch: 275, Loss: 0.2952486574649811
Epoch: 2, Batch: 276, Loss: 0.8290866613388062
Epoch: 2, Batch: 277, Loss: 0.4751574397087097
Epoch: 2, Batch: 278, Loss: 0.7655386924743652
Epoch: 2, Batch: 279, Loss: 0.28876057267189026
Epoch: 2, Batch: 280, Loss: 0.519136905670166
Epoch: 2, Batch: 281, Loss: 1.002949595451355
Epoch: 2, Batch: 282, Loss: 0.8727221488952637
Epoch: 2, Batch: 283, Loss: 1.0275908708572388
Epoch: 2, Batch: 284, Loss: 0.7384412288665771
Epoch: 2, Batch: 285, Loss: 0.9150089025497437
Epoch: 2, Batch: 286, Loss: 0.5968385934829712
Epoch: 2, Batch: 287, Loss: 0.3742843568325043
Epoch: 2, Batch: 288, Loss: 0.7433366179466248
Epoch: 2, Batch: 289, Loss: 0.5771291851997375
Epoch: 2, Batch: 290, Loss: 0.2911244034767151
Epoch: 2, Batch: 291, Loss: 0.9278554916381836
Epoch: 2, Batch: 292, Loss: 0.43863487243652344
Epoch: 2, Batch: 293, Loss: 0.32755717635154724
Epoch: 2, Batch: 294, Loss: 0.3873955309391022
Epoch: 2, Batch: 295, Loss: 0.2724040448665619
Epoch: 2, Batch: 296, Loss: 0.2631125748157501
Epoch: 2, Batch: 297, Loss: 0.29018229246139526
Epoch: 2, Batch: 298, Loss: 0.2714303135871887
Epoch: 2, Batch: 299, Loss: 0.3750799894332886
Epoch: 2, Batch: 300, Loss: 0.7275680899620056
Epoch: 2, Batch: 301, Loss: 0.4612502455711365
Epoch: 2, Batch: 302, Loss: 0.5571292042732239
Epoch: 2, Batch: 303, Loss: 0.72721266746521
Epoch: 2, Batch: 304, Loss: 0.6097157597541809
Epoch: 2, Batch: 305, Loss: 0.797051191329956
Epoch: 2, Batch: 306, Loss: 0.703596293926239
Epoch: 2, Batch: 307, Loss: 0.4943815767765045
Epoch: 2, Batch: 308, Loss: 0.7572486996650696
Epoch: 2, Batch: 309, Loss: 0.8231921195983887
Epoch: 2, Batch: 310, Loss: 0.8067253828048706
Epoch Completed: 2/10, Time: 212.80500197410583, Train Loss: 0.5959273890141518, Valid Loss: 0.5842985039757144

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:14  model_time: 0.3216 (0.3216)  evaluator_time: 0.0073 (0.0073)  time: 0.3663  data: 0.0343  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3285 (0.3241)  evaluator_time: 0.0054 (0.0068)  time: 0.3635  data: 0.0339  max mem: 4996
Test: Total time: 0:00:14 (0.3690 s / it)
Averaged stats: model_time: 0.3285 (0.3241)  evaluator_time: 0.0054 (0.0068)
Accumulating evaluation results...
DONE (t=0.06s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.086
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.007
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.056
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.084
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.230
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.425
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.225
Epoch: 3, Batch: 311, Loss: 0.7797452807426453
Epoch: 3, Batch: 312, Loss: 0.6209969520568848
Epoch: 3, Batch: 313, Loss: 0.7527624368667603
Epoch: 3, Batch: 314, Loss: 0.4799514412879944
Epoch: 3, Batch: 315, Loss: 0.33427000045776367
Epoch: 3, Batch: 316, Loss: 0.34676021337509155
Epoch: 3, Batch: 317, Loss: 0.6705266833305359
Epoch: 3, Batch: 318, Loss: 0.33501073718070984
Epoch: 3, Batch: 319, Loss: 0.43760475516319275
Epoch: 3, Batch: 320, Loss: 0.4317333698272705
Epoch: 3, Batch: 321, Loss: 0.3827773332595825
Epoch: 3, Batch: 322, Loss: 0.29531311988830566
Epoch: 3, Batch: 323, Loss: 0.25694891810417175
Epoch: 3, Batch: 324, Loss: 0.36967340111732483
Epoch: 3, Batch: 325, Loss: 0.3938140571117401
Epoch: 3, Batch: 326, Loss: 0.20847535133361816
Epoch: 3, Batch: 327, Loss: 1.7269666194915771
Epoch: 3, Batch: 328, Loss: 0.30857571959495544
Epoch: 3, Batch: 329, Loss: 0.37185531854629517
Epoch: 3, Batch: 330, Loss: 0.8396599292755127
Epoch: 3, Batch: 331, Loss: 0.5593007206916809
Epoch: 3, Batch: 332, Loss: 0.8634951114654541
Epoch: 3, Batch: 333, Loss: 0.3954976201057434
Epoch: 3, Batch: 334, Loss: 0.7138503193855286
Epoch: 3, Batch: 335, Loss: 0.6048873662948608
Epoch: 3, Batch: 336, Loss: 0.3919481337070465
Epoch: 3, Batch: 337, Loss: 0.4415210485458374
Epoch: 3, Batch: 338, Loss: 0.6769468188285828
Epoch: 3, Batch: 339, Loss: 0.60385662317276
Epoch: 3, Batch: 340, Loss: 0.5945044159889221
Epoch: 3, Batch: 341, Loss: 1.9008827209472656
Epoch: 3, Batch: 342, Loss: 0.7325576543807983
Epoch: 3, Batch: 343, Loss: 0.5066179037094116
Epoch: 3, Batch: 344, Loss: 0.8425310850143433
Epoch: 3, Batch: 345, Loss: 0.39552369713783264
Epoch: 3, Batch: 346, Loss: 0.4171315133571625
Epoch: 3, Batch: 347, Loss: 0.8107242584228516
Epoch: 3, Batch: 348, Loss: 0.31336715817451477
Epoch: 3, Batch: 349, Loss: 0.3159395158290863
Epoch: 3, Batch: 350, Loss: 0.8581705093383789
Epoch: 3, Batch: 351, Loss: 0.5174458026885986
Epoch: 3, Batch: 352, Loss: 0.43610671162605286
Epoch: 3, Batch: 353, Loss: 0.395976185798645
Epoch: 3, Batch: 354, Loss: 0.5072202086448669
Epoch: 3, Batch: 355, Loss: 0.4842100441455841
Epoch: 3, Batch: 356, Loss: 0.3726406693458557
Epoch: 3, Batch: 357, Loss: 0.31227245926856995
Epoch: 3, Batch: 358, Loss: 0.3418475389480591
Epoch: 3, Batch: 359, Loss: 0.6797202825546265
Epoch: 3, Batch: 360, Loss: 0.593215823173523
Epoch: 3, Batch: 361, Loss: 0.5249460935592651
Epoch: 3, Batch: 362, Loss: 1.0427982807159424
Epoch: 3, Batch: 363, Loss: 0.8185739517211914
Epoch: 3, Batch: 364, Loss: 0.4236323833465576
Epoch: 3, Batch: 365, Loss: 0.4404831528663635
Epoch: 3, Batch: 366, Loss: 0.7269351482391357
Epoch: 3, Batch: 367, Loss: 0.4572409987449646
Epoch: 3, Batch: 368, Loss: 0.405924916267395
Epoch: 3, Batch: 369, Loss: 0.43155810236930847
Epoch: 3, Batch: 370, Loss: 0.46982285380363464
Epoch: 3, Batch: 371, Loss: 0.3722327649593353
Epoch: 3, Batch: 372, Loss: 0.4671507775783539
Epoch: 3, Batch: 373, Loss: 0.4893745481967926
Epoch: 3, Batch: 374, Loss: 0.4135088324546814
Epoch: 3, Batch: 375, Loss: 0.4909574091434479
Epoch: 3, Batch: 376, Loss: 0.40200963616371155
Epoch: 3, Batch: 377, Loss: 0.7741833925247192
Epoch: 3, Batch: 378, Loss: 0.6940475702285767
Epoch: 3, Batch: 379, Loss: 0.5930858850479126
Epoch: 3, Batch: 380, Loss: 0.631896436214447
Epoch: 3, Batch: 381, Loss: 0.584481954574585
Epoch: 3, Batch: 382, Loss: 0.7513266801834106
Epoch: 3, Batch: 383, Loss: 0.5240161418914795
Epoch: 3, Batch: 384, Loss: 0.38744866847991943
Epoch: 3, Batch: 385, Loss: 0.39054644107818604
Epoch: 3, Batch: 386, Loss: 0.4467940926551819
Epoch: 3, Batch: 387, Loss: 0.28899329900741577
Epoch: 3, Batch: 388, Loss: 1.216874122619629
Epoch: 3, Batch: 389, Loss: 0.33030474185943604
Epoch: 3, Batch: 390, Loss: 0.2874280512332916
Epoch: 3, Batch: 391, Loss: 0.5950227379798889
Epoch: 3, Batch: 392, Loss: 0.24812714755535126
Epoch: 3, Batch: 393, Loss: 0.31191352009773254
Epoch: 3, Batch: 394, Loss: 0.931393027305603
Epoch: 3, Batch: 395, Loss: 1.0007946491241455
Epoch: 3, Batch: 396, Loss: 1.4356451034545898
Epoch: 3, Batch: 397, Loss: 0.3066805303096771
Epoch: 3, Batch: 398, Loss: 0.41110628843307495
Epoch: 3, Batch: 399, Loss: 0.3970019817352295
Epoch: 3, Batch: 400, Loss: 0.5566486120223999
Epoch: 3, Batch: 401, Loss: 0.47555550932884216
Epoch: 3, Batch: 402, Loss: 0.5161989331245422
Epoch: 3, Batch: 403, Loss: 0.5182144641876221
Epoch: 3, Batch: 404, Loss: 0.32404568791389465
Epoch: 3, Batch: 405, Loss: 0.710081934928894
Epoch: 3, Batch: 406, Loss: 0.35417109727859497
Epoch: 3, Batch: 407, Loss: 0.326019287109375
Epoch: 3, Batch: 408, Loss: 0.8961617350578308
Epoch: 3, Batch: 409, Loss: 0.6214137077331543
Epoch: 3, Batch: 410, Loss: 0.6363965272903442
Epoch: 3, Batch: 411, Loss: 0.37824323773384094
Epoch: 3, Batch: 412, Loss: 1.0015113353729248
Epoch: 3, Batch: 413, Loss: 0.4641883969306946
Epoch: 3, Batch: 414, Loss: 0.4640539288520813
Epoch: 3, Batch: 415, Loss: 0.46467068791389465
Epoch: 3, Batch: 416, Loss: 0.3477603495121002
Epoch: 3, Batch: 417, Loss: 0.7667384743690491
Epoch: 3, Batch: 418, Loss: 0.3425493538379669
Epoch: 3, Batch: 419, Loss: 0.4417591691017151
Epoch: 3, Batch: 420, Loss: 0.3057403564453125
Epoch: 3, Batch: 421, Loss: 0.3176383674144745
Epoch: 3, Batch: 422, Loss: 0.32428884506225586
Epoch: 3, Batch: 423, Loss: 0.28979241847991943
Epoch: 3, Batch: 424, Loss: 0.18815435469150543
Epoch: 3, Batch: 425, Loss: 0.24317525327205658
Epoch: 3, Batch: 426, Loss: 0.2900397777557373
Epoch: 3, Batch: 427, Loss: 0.32019609212875366
Epoch: 3, Batch: 428, Loss: 0.2491292804479599
Epoch: 3, Batch: 429, Loss: 0.3747209906578064
Epoch: 3, Batch: 430, Loss: 0.25597062706947327
Epoch: 3, Batch: 431, Loss: 0.6789043545722961
Epoch: 3, Batch: 432, Loss: 0.3935454487800598
Epoch: 3, Batch: 433, Loss: 0.6394526958465576
Epoch: 3, Batch: 434, Loss: 0.2910560965538025
Epoch: 3, Batch: 435, Loss: 0.4335835874080658
Epoch: 3, Batch: 436, Loss: 0.9125944375991821
Epoch: 3, Batch: 437, Loss: 0.6778860092163086
Epoch: 3, Batch: 438, Loss: 0.97652268409729
Epoch: 3, Batch: 439, Loss: 0.6586195230484009
Epoch: 3, Batch: 440, Loss: 0.8439406752586365
Epoch: 3, Batch: 441, Loss: 0.5910263061523438
Epoch: 3, Batch: 442, Loss: 0.3642999231815338
Epoch: 3, Batch: 443, Loss: 0.6073695421218872
Epoch: 3, Batch: 444, Loss: 0.6137658357620239
Epoch: 3, Batch: 445, Loss: 0.2742445170879364
Epoch: 3, Batch: 446, Loss: 0.7930351495742798
Epoch: 3, Batch: 447, Loss: 0.4168584942817688
Epoch: 3, Batch: 448, Loss: 0.2853129208087921
Epoch: 3, Batch: 449, Loss: 0.34100714325904846
Epoch: 3, Batch: 450, Loss: 0.23388904333114624
Epoch: 3, Batch: 451, Loss: 0.25897642970085144
Epoch: 3, Batch: 452, Loss: 0.26656293869018555
Epoch: 3, Batch: 453, Loss: 0.26818808913230896
Epoch: 3, Batch: 454, Loss: 0.32693374156951904
Epoch: 3, Batch: 455, Loss: 0.861690878868103
Epoch: 3, Batch: 456, Loss: 0.45151641964912415
Epoch: 3, Batch: 457, Loss: 0.3971370458602905
Epoch: 3, Batch: 458, Loss: 0.6857611536979675
Epoch: 3, Batch: 459, Loss: 0.6642859578132629
Epoch: 3, Batch: 460, Loss: 0.5996744632720947
Epoch: 3, Batch: 461, Loss: 0.5139524936676025
Epoch: 3, Batch: 462, Loss: 0.37106335163116455
Epoch: 3, Batch: 463, Loss: 0.4692259430885315
Epoch: 3, Batch: 464, Loss: 0.6709169149398804
Epoch: 3, Batch: 465, Loss: 0.7014182209968567
Epoch Completed: 3/10, Time: 212.81399536132812, Train Loss: 0.5299292973933681, Valid Loss: 0.5043278467270635

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:14  model_time: 0.3172 (0.3172)  evaluator_time: 0.0057 (0.0057)  time: 0.3604  data: 0.0344  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3264 (0.3220)  evaluator_time: 0.0044 (0.0051)  time: 0.3604  data: 0.0339  max mem: 4996
Test: Total time: 0:00:14 (0.3651 s / it)
Averaged stats: model_time: 0.3264 (0.3220)  evaluator_time: 0.0044 (0.0051)
Accumulating evaluation results...
DONE (t=0.05s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.078
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.059
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.186
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.186
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.217
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.186
Epoch: 4, Batch: 466, Loss: 0.7075049877166748
Epoch: 4, Batch: 467, Loss: 0.5798059701919556
Epoch: 4, Batch: 468, Loss: 0.8233840465545654
Epoch: 4, Batch: 469, Loss: 0.41614261269569397
Epoch: 4, Batch: 470, Loss: 0.3193436563014984
Epoch: 4, Batch: 471, Loss: 0.29323121905326843
Epoch: 4, Batch: 472, Loss: 0.5903343558311462
Epoch: 4, Batch: 473, Loss: 0.2533009648323059
Epoch: 4, Batch: 474, Loss: 0.4110505282878876
Epoch: 4, Batch: 475, Loss: 0.34952497482299805
Epoch: 4, Batch: 476, Loss: 0.32987409830093384
Epoch: 4, Batch: 477, Loss: 0.2683294415473938
Epoch: 4, Batch: 478, Loss: 0.25798869132995605
Epoch: 4, Batch: 479, Loss: 0.3526970446109772
Epoch: 4, Batch: 480, Loss: 0.3556334674358368
Epoch: 4, Batch: 481, Loss: 0.15584474802017212
Epoch: 4, Batch: 482, Loss: 1.656677484512329
Epoch: 4, Batch: 483, Loss: 0.2795337438583374
Epoch: 4, Batch: 484, Loss: 0.3801788091659546
Epoch: 4, Batch: 485, Loss: 0.810634970664978
Epoch: 4, Batch: 486, Loss: 0.5430583357810974
Epoch: 4, Batch: 487, Loss: 0.7885994911193848
Epoch: 4, Batch: 488, Loss: 0.368115097284317
Epoch: 4, Batch: 489, Loss: 0.6164621114730835
Epoch: 4, Batch: 490, Loss: 0.4977620840072632
Epoch: 4, Batch: 491, Loss: 0.36788249015808105
Epoch: 4, Batch: 492, Loss: 0.35982656478881836
Epoch: 4, Batch: 493, Loss: 0.6484178304672241
Epoch: 4, Batch: 494, Loss: 0.5112186074256897
Epoch: 4, Batch: 495, Loss: 0.4590475261211395
Epoch: 4, Batch: 496, Loss: 1.735151767730713
Epoch: 4, Batch: 497, Loss: 0.5652199387550354
Epoch: 4, Batch: 498, Loss: 0.601481556892395
Epoch: 4, Batch: 499, Loss: 0.9347788691520691
Epoch: 4, Batch: 500, Loss: 0.32194784283638
Epoch: 4, Batch: 501, Loss: 0.38321492075920105
Epoch: 4, Batch: 502, Loss: 0.7694518566131592
Epoch: 4, Batch: 503, Loss: 0.2512257993221283
Epoch: 4, Batch: 504, Loss: 0.25586241483688354
Epoch: 4, Batch: 505, Loss: 0.8238687515258789
Epoch: 4, Batch: 506, Loss: 0.41672995686531067
Epoch: 4, Batch: 507, Loss: 0.4693019986152649
Epoch: 4, Batch: 508, Loss: 0.42538386583328247
Epoch: 4, Batch: 509, Loss: 0.43467143177986145
Epoch: 4, Batch: 510, Loss: 0.3977888822555542
Epoch: 4, Batch: 511, Loss: 0.3652288317680359
Epoch: 4, Batch: 512, Loss: 0.3325173556804657
Epoch: 4, Batch: 513, Loss: 0.36576569080352783
Epoch: 4, Batch: 514, Loss: 0.5942301750183105
Epoch: 4, Batch: 515, Loss: 0.4348180890083313
Epoch: 4, Batch: 516, Loss: 0.494693785905838
Epoch: 4, Batch: 517, Loss: 0.9308162927627563
Epoch: 4, Batch: 518, Loss: 0.6542593240737915
Epoch: 4, Batch: 519, Loss: 0.38155293464660645
Epoch: 4, Batch: 520, Loss: 0.43041756749153137
Epoch: 4, Batch: 521, Loss: 0.5235879421234131
Epoch: 4, Batch: 522, Loss: 0.39670491218566895
Epoch: 4, Batch: 523, Loss: 0.36232078075408936
Epoch: 4, Batch: 524, Loss: 0.40528273582458496
Epoch: 4, Batch: 525, Loss: 0.40241673588752747
Epoch: 4, Batch: 526, Loss: 0.3436940014362335
Epoch: 4, Batch: 527, Loss: 0.41742831468582153
Epoch: 4, Batch: 528, Loss: 0.6758478283882141
Epoch: 4, Batch: 529, Loss: 0.43234753608703613
Epoch: 4, Batch: 530, Loss: 0.3782801032066345
Epoch: 4, Batch: 531, Loss: 0.3013254404067993
Epoch: 4, Batch: 532, Loss: 0.6435999870300293
Epoch: 4, Batch: 533, Loss: 0.6023685932159424
Epoch: 4, Batch: 534, Loss: 0.47933605313301086
Epoch: 4, Batch: 535, Loss: 0.5774673819541931
Epoch: 4, Batch: 536, Loss: 0.6484385132789612
Epoch: 4, Batch: 537, Loss: 0.6786652207374573
Epoch: 4, Batch: 538, Loss: 0.43463295698165894
Epoch: 4, Batch: 539, Loss: 0.36802008748054504
Epoch: 4, Batch: 540, Loss: 0.3206437826156616
Epoch: 4, Batch: 541, Loss: 0.36912721395492554
Epoch: 4, Batch: 542, Loss: 0.27422991394996643
Epoch: 4, Batch: 543, Loss: 1.1784125566482544
Epoch: 4, Batch: 544, Loss: 0.28222066164016724
Epoch: 4, Batch: 545, Loss: 0.26658618450164795
Epoch: 4, Batch: 546, Loss: 0.4955756664276123
Epoch: 4, Batch: 547, Loss: 0.2910318374633789
Epoch: 4, Batch: 548, Loss: 0.2763065695762634
Epoch: 4, Batch: 549, Loss: 0.7274717092514038
Epoch: 4, Batch: 550, Loss: 0.8757940530776978
Epoch: 4, Batch: 551, Loss: 1.0416364669799805
Epoch: 4, Batch: 552, Loss: 0.2983091175556183
Epoch: 4, Batch: 553, Loss: 0.37457478046417236
Epoch: 4, Batch: 554, Loss: 0.37740379571914673
Epoch: 4, Batch: 555, Loss: 0.5306004881858826
Epoch: 4, Batch: 556, Loss: 0.41854074597358704
Epoch: 4, Batch: 557, Loss: 0.43301519751548767
Epoch: 4, Batch: 558, Loss: 0.44857507944107056
Epoch: 4, Batch: 559, Loss: 0.24293112754821777
Epoch: 4, Batch: 560, Loss: 0.5010302066802979
Epoch: 4, Batch: 561, Loss: 0.3297465443611145
Epoch: 4, Batch: 562, Loss: 0.3131920397281647
Epoch: 4, Batch: 563, Loss: 0.7572541236877441
Epoch: 4, Batch: 564, Loss: 0.528226375579834
Epoch: 4, Batch: 565, Loss: 0.49498414993286133
Epoch: 4, Batch: 566, Loss: 0.39010632038116455
Epoch: 4, Batch: 567, Loss: 0.8280268907546997
Epoch: 4, Batch: 568, Loss: 0.4599190354347229
Epoch: 4, Batch: 569, Loss: 0.3293595612049103
Epoch: 4, Batch: 570, Loss: 0.28803887963294983
Epoch: 4, Batch: 571, Loss: 0.4323697090148926
Epoch: 4, Batch: 572, Loss: 0.7440519332885742
Epoch: 4, Batch: 573, Loss: 0.319959819316864
Epoch: 4, Batch: 574, Loss: 0.40263473987579346
Epoch: 4, Batch: 575, Loss: 0.3217984437942505
Epoch: 4, Batch: 576, Loss: 0.389395147562027
Epoch: 4, Batch: 577, Loss: 0.31687164306640625
Epoch: 4, Batch: 578, Loss: 0.278996080160141
Epoch: 4, Batch: 579, Loss: 0.15731669962406158
Epoch: 4, Batch: 580, Loss: 0.23507393896579742
Epoch: 4, Batch: 581, Loss: 0.2868642508983612
Epoch: 4, Batch: 582, Loss: 0.26325634121894836
Epoch: 4, Batch: 583, Loss: 0.23086577653884888
Epoch: 4, Batch: 584, Loss: 0.36667442321777344
Epoch: 4, Batch: 585, Loss: 0.23838090896606445
Epoch: 4, Batch: 586, Loss: 0.5589333176612854
Epoch: 4, Batch: 587, Loss: 0.3834298849105835
Epoch: 4, Batch: 588, Loss: 0.4831583499908447
Epoch: 4, Batch: 589, Loss: 0.26081958413124084
Epoch: 4, Batch: 590, Loss: 0.42995011806488037
Epoch: 4, Batch: 591, Loss: 0.7696487307548523
Epoch: 4, Batch: 592, Loss: 0.5658701658248901
Epoch: 4, Batch: 593, Loss: 0.7708485126495361
Epoch: 4, Batch: 594, Loss: 0.5202118158340454
Epoch: 4, Batch: 595, Loss: 0.597008228302002
Epoch: 4, Batch: 596, Loss: 0.45007073879241943
Epoch: 4, Batch: 597, Loss: 0.3538599908351898
Epoch: 4, Batch: 598, Loss: 0.4616602063179016
Epoch: 4, Batch: 599, Loss: 0.38097864389419556
Epoch: 4, Batch: 600, Loss: 0.22316581010818481
Epoch: 4, Batch: 601, Loss: 0.7123128771781921
Epoch: 4, Batch: 602, Loss: 0.4128050208091736
Epoch: 4, Batch: 603, Loss: 0.22449900209903717
Epoch: 4, Batch: 604, Loss: 0.31205064058303833
Epoch: 4, Batch: 605, Loss: 0.2058221697807312
Epoch: 4, Batch: 606, Loss: 0.2253415733575821
Epoch: 4, Batch: 607, Loss: 0.22022128105163574
Epoch: 4, Batch: 608, Loss: 0.2362513244152069
Epoch: 4, Batch: 609, Loss: 0.28427907824516296
Epoch: 4, Batch: 610, Loss: 0.6852952241897583
Epoch: 4, Batch: 611, Loss: 0.3844975233078003
Epoch: 4, Batch: 612, Loss: 0.4183933138847351
Epoch: 4, Batch: 613, Loss: 0.6109894514083862
Epoch: 4, Batch: 614, Loss: 0.42017078399658203
Epoch: 4, Batch: 615, Loss: 0.540683925151825
Epoch: 4, Batch: 616, Loss: 0.4193264842033386
Epoch: 4, Batch: 617, Loss: 0.39301615953445435
Epoch: 4, Batch: 618, Loss: 0.4486575722694397
Epoch: 4, Batch: 619, Loss: 0.594944417476654
Epoch: 4, Batch: 620, Loss: 0.4651171863079071
Epoch Completed: 4/10, Time: 213.0455949306488, Train Loss: 0.46932400657284645, Valid Loss: 0.4639255593861303

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:15  model_time: 0.3309 (0.3309)  evaluator_time: 0.0075 (0.0075)  time: 0.3760  data: 0.0345  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3256 (0.3223)  evaluator_time: 0.0049 (0.0062)  time: 0.3605  data: 0.0343  max mem: 4996
Test: Total time: 0:00:14 (0.3669 s / it)
Averaged stats: model_time: 0.3256 (0.3223)  evaluator_time: 0.0049 (0.0062)
Accumulating evaluation results...
DONE (t=0.06s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.058
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.021
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.039
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.060
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.178
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.181
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.442
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.171
Epoch: 5, Batch: 621, Loss: 0.6103659868240356
Epoch: 5, Batch: 622, Loss: 0.5536967515945435
Epoch: 5, Batch: 623, Loss: 0.5420207977294922
Epoch: 5, Batch: 624, Loss: 0.3583410382270813
Epoch: 5, Batch: 625, Loss: 0.3003775179386139
Epoch: 5, Batch: 626, Loss: 0.35563454031944275
Epoch: 5, Batch: 627, Loss: 0.560655415058136
Epoch: 5, Batch: 628, Loss: 0.28299427032470703
Epoch: 5, Batch: 629, Loss: 0.3850441575050354
Epoch: 5, Batch: 630, Loss: 0.2958831489086151
Epoch: 5, Batch: 631, Loss: 0.2880915105342865
Epoch: 5, Batch: 632, Loss: 0.23491042852401733
Epoch: 5, Batch: 633, Loss: 0.18944615125656128
Epoch: 5, Batch: 634, Loss: 0.3321472108364105
Epoch: 5, Batch: 635, Loss: 0.32766956090927124
Epoch: 5, Batch: 636, Loss: 0.16315750777721405
Epoch: 5, Batch: 637, Loss: 1.6950502395629883
Epoch: 5, Batch: 638, Loss: 0.2514781057834625
Epoch: 5, Batch: 639, Loss: 0.3249070942401886
Epoch: 5, Batch: 640, Loss: 0.6691994667053223
Epoch: 5, Batch: 641, Loss: 0.501941978931427
Epoch: 5, Batch: 642, Loss: 0.7893871665000916
Epoch: 5, Batch: 643, Loss: 0.3565189838409424
Epoch: 5, Batch: 644, Loss: 0.5237781405448914
Epoch: 5, Batch: 645, Loss: 0.4778585731983185
Epoch: 5, Batch: 646, Loss: 0.31316301226615906
Epoch: 5, Batch: 647, Loss: 0.2982783019542694
Epoch: 5, Batch: 648, Loss: 0.6100059151649475
Epoch: 5, Batch: 649, Loss: 0.32405874133110046
Epoch: 5, Batch: 650, Loss: 0.3771393895149231
Epoch: 5, Batch: 651, Loss: 1.3747379779815674
Epoch: 5, Batch: 652, Loss: 0.6121495366096497
Epoch: 5, Batch: 653, Loss: 0.2799082100391388
Epoch: 5, Batch: 654, Loss: 0.7519410252571106
Epoch: 5, Batch: 655, Loss: 0.30524030327796936
Epoch: 5, Batch: 656, Loss: 0.35655683279037476
Epoch: 5, Batch: 657, Loss: 0.662850022315979
Epoch: 5, Batch: 658, Loss: 0.3813142776489258
Epoch: 5, Batch: 659, Loss: 0.2385287582874298
Epoch: 5, Batch: 660, Loss: 0.675601601600647
Epoch: 5, Batch: 661, Loss: 0.35791823267936707
Epoch: 5, Batch: 662, Loss: 0.41545748710632324
Epoch: 5, Batch: 663, Loss: 0.34959676861763
Epoch: 5, Batch: 664, Loss: 0.4176323711872101
Epoch: 5, Batch: 665, Loss: 0.35881710052490234
Epoch: 5, Batch: 666, Loss: 0.3312058448791504
Epoch: 5, Batch: 667, Loss: 0.28700876235961914
Epoch: 5, Batch: 668, Loss: 0.3418159782886505
Epoch: 5, Batch: 669, Loss: 0.5058761239051819
Epoch: 5, Batch: 670, Loss: 0.3449989855289459
Epoch: 5, Batch: 671, Loss: 0.355925977230072
Epoch: 5, Batch: 672, Loss: 0.7984791994094849
Epoch: 5, Batch: 673, Loss: 0.586522102355957
Epoch: 5, Batch: 674, Loss: 0.32122504711151123
Epoch: 5, Batch: 675, Loss: 0.37089109420776367
Epoch: 5, Batch: 676, Loss: 0.5395867228507996
Epoch: 5, Batch: 677, Loss: 0.2521679699420929
Epoch: 5, Batch: 678, Loss: 0.3877026438713074
Epoch: 5, Batch: 679, Loss: 0.33236584067344666
Epoch: 5, Batch: 680, Loss: 0.3676619827747345
Epoch: 5, Batch: 681, Loss: 0.3329523205757141
Epoch: 5, Batch: 682, Loss: 0.3448616564273834
Epoch: 5, Batch: 683, Loss: 0.394306480884552
Epoch: 5, Batch: 684, Loss: 0.3977777063846588
Epoch: 5, Batch: 685, Loss: 0.3321257531642914
Epoch: 5, Batch: 686, Loss: 0.24381422996520996
Epoch: 5, Batch: 687, Loss: 0.616481602191925
Epoch: 5, Batch: 688, Loss: 0.514175534248352
Epoch: 5, Batch: 689, Loss: 0.39805343747138977
Epoch: 5, Batch: 690, Loss: 0.5046164989471436
Epoch: 5, Batch: 691, Loss: 0.5179386734962463
Epoch: 5, Batch: 692, Loss: 0.5471802353858948
Epoch: 5, Batch: 693, Loss: 0.4945870041847229
Epoch: 5, Batch: 694, Loss: 0.3238767683506012
Epoch: 5, Batch: 695, Loss: 0.31427839398384094
Epoch: 5, Batch: 696, Loss: 0.349562406539917
Epoch: 5, Batch: 697, Loss: 0.24130459129810333
Epoch: 5, Batch: 698, Loss: 1.06839919090271
Epoch: 5, Batch: 699, Loss: 0.2580691874027252
Epoch: 5, Batch: 700, Loss: 0.21810196340084076
Epoch: 5, Batch: 701, Loss: 0.47588083148002625
Epoch: 5, Batch: 702, Loss: 0.24744708836078644
Epoch: 5, Batch: 703, Loss: 0.244825541973114
Epoch: 5, Batch: 704, Loss: 0.6189020276069641
Epoch: 5, Batch: 705, Loss: 0.7996156811714172
Epoch: 5, Batch: 706, Loss: 1.1698133945465088
Epoch: 5, Batch: 707, Loss: 0.2782834768295288
Epoch: 5, Batch: 708, Loss: 0.3737346827983856
Epoch: 5, Batch: 709, Loss: 0.46540647745132446
Epoch: 5, Batch: 710, Loss: 0.4921119511127472
Epoch: 5, Batch: 711, Loss: 0.39364612102508545
Epoch: 5, Batch: 712, Loss: 0.39545029401779175
Epoch: 5, Batch: 713, Loss: 0.38010454177856445
Epoch: 5, Batch: 714, Loss: 0.27814292907714844
Epoch: 5, Batch: 715, Loss: 0.62755286693573
Epoch: 5, Batch: 716, Loss: 0.3202994465827942
Epoch: 5, Batch: 717, Loss: 0.2897045612335205
Epoch: 5, Batch: 718, Loss: 0.6243623495101929
Epoch: 5, Batch: 719, Loss: 0.6995207667350769
Epoch: 5, Batch: 720, Loss: 0.503407895565033
Epoch: 5, Batch: 721, Loss: 0.3103707432746887
Epoch: 5, Batch: 722, Loss: 0.9271420240402222
Epoch: 5, Batch: 723, Loss: 0.385714054107666
Epoch: 5, Batch: 724, Loss: 0.3444192707538605
Epoch: 5, Batch: 725, Loss: 0.29727840423583984
Epoch: 5, Batch: 726, Loss: 0.27000877261161804
Epoch: 5, Batch: 727, Loss: 0.6611136794090271
Epoch: 5, Batch: 728, Loss: 0.309145987033844
Epoch: 5, Batch: 729, Loss: 0.3488447070121765
Epoch: 5, Batch: 730, Loss: 0.2821464240550995
Epoch: 5, Batch: 731, Loss: 0.3192398250102997
Epoch: 5, Batch: 732, Loss: 0.32716742157936096
Epoch: 5, Batch: 733, Loss: 0.27524513006210327
Epoch: 5, Batch: 734, Loss: 0.16864009201526642
Epoch: 5, Batch: 735, Loss: 0.24055859446525574
Epoch: 5, Batch: 736, Loss: 0.2556273937225342
Epoch: 5, Batch: 737, Loss: 0.23494325578212738
Epoch: 5, Batch: 738, Loss: 0.1884395182132721
Epoch: 5, Batch: 739, Loss: 0.2628703713417053
Epoch: 5, Batch: 740, Loss: 0.22897455096244812
Epoch: 5, Batch: 741, Loss: 0.44478732347488403
Epoch: 5, Batch: 742, Loss: 0.3413480222225189
Epoch: 5, Batch: 743, Loss: 0.4529252052307129
Epoch: 5, Batch: 744, Loss: 0.22604063153266907
Epoch: 5, Batch: 745, Loss: 0.39278632402420044
Epoch: 5, Batch: 746, Loss: 0.6467168927192688
Epoch: 5, Batch: 747, Loss: 0.48391443490982056
Epoch: 5, Batch: 748, Loss: 0.8110119104385376
Epoch: 5, Batch: 749, Loss: 0.45337021350860596
Epoch: 5, Batch: 750, Loss: 0.5867133140563965
Epoch: 5, Batch: 751, Loss: 0.43641334772109985
Epoch: 5, Batch: 752, Loss: 0.3451511263847351
Epoch: 5, Batch: 753, Loss: 0.4691818058490753
Epoch: 5, Batch: 754, Loss: 0.31578099727630615
Epoch: 5, Batch: 755, Loss: 0.24782417714595795
Epoch: 5, Batch: 756, Loss: 0.5475621223449707
Epoch: 5, Batch: 757, Loss: 0.3189115524291992
Epoch: 5, Batch: 758, Loss: 0.21537382900714874
Epoch: 5, Batch: 759, Loss: 0.2585136294364929
Epoch: 5, Batch: 760, Loss: 0.1481226235628128
Epoch: 5, Batch: 761, Loss: 0.19776982069015503
Epoch: 5, Batch: 762, Loss: 0.20569708943367004
Epoch: 5, Batch: 763, Loss: 0.23087584972381592
Epoch: 5, Batch: 764, Loss: 0.2788688838481903
Epoch: 5, Batch: 765, Loss: 0.4439450800418854
Epoch: 5, Batch: 766, Loss: 0.3717808425426483
Epoch: 5, Batch: 767, Loss: 0.29782915115356445
Epoch: 5, Batch: 768, Loss: 0.5886021852493286
Epoch: 5, Batch: 769, Loss: 0.3540648818016052
Epoch: 5, Batch: 770, Loss: 0.47198066115379333
Epoch: 5, Batch: 771, Loss: 0.4037199914455414
Epoch: 5, Batch: 772, Loss: 0.29961881041526794
Epoch: 5, Batch: 773, Loss: 0.35823148488998413
Epoch: 5, Batch: 774, Loss: 0.5413749814033508
Epoch: 5, Batch: 775, Loss: 0.44642210006713867
Epoch Completed: 5/10, Time: 212.91692638397217, Train Loss: 0.42139796105123334, Valid Loss: 0.4694873003229018

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:14  model_time: 0.3254 (0.3254)  evaluator_time: 0.0040 (0.0040)  time: 0.3670  data: 0.0346  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3272 (0.3230)  evaluator_time: 0.0041 (0.0049)  time: 0.3614  data: 0.0343  max mem: 4996
Test: Total time: 0:00:14 (0.3664 s / it)
Averaged stats: model_time: 0.3272 (0.3230)  evaluator_time: 0.0041 (0.0049)
Accumulating evaluation results...
DONE (t=0.05s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.062
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.018
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.067
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.219
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.221
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.125
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.224
Epoch: 6, Batch: 776, Loss: 0.7016833424568176
Epoch: 6, Batch: 777, Loss: 0.532598614692688
Epoch: 6, Batch: 778, Loss: 0.5912841558456421
Epoch: 6, Batch: 779, Loss: 0.3312954306602478
Epoch: 6, Batch: 780, Loss: 0.2673775851726532
Epoch: 6, Batch: 781, Loss: 0.297666996717453
Epoch: 6, Batch: 782, Loss: 0.503557026386261
Epoch: 6, Batch: 783, Loss: 0.24833984673023224
Epoch: 6, Batch: 784, Loss: 0.3192405104637146
Epoch: 6, Batch: 785, Loss: 0.3589819073677063
Epoch: 6, Batch: 786, Loss: 0.2951895296573639
Epoch: 6, Batch: 787, Loss: 0.2192266881465912
Epoch: 6, Batch: 788, Loss: 0.18706463277339935
Epoch: 6, Batch: 789, Loss: 0.29426273703575134
Epoch: 6, Batch: 790, Loss: 0.3287423253059387
Epoch: 6, Batch: 791, Loss: 0.10867048054933548
Epoch: 6, Batch: 792, Loss: 1.625368595123291
Epoch: 6, Batch: 793, Loss: 0.23891635239124298
Epoch: 6, Batch: 794, Loss: 0.30834242701530457
Epoch: 6, Batch: 795, Loss: 1.050762414932251
Epoch: 6, Batch: 796, Loss: 0.43093863129615784
Epoch: 6, Batch: 797, Loss: 0.8780426979064941
Epoch: 6, Batch: 798, Loss: 0.2994060516357422
Epoch: 6, Batch: 799, Loss: 0.43373656272888184
Epoch: 6, Batch: 800, Loss: 0.41526564955711365
Epoch: 6, Batch: 801, Loss: 0.2945444881916046
Epoch: 6, Batch: 802, Loss: 0.36272865533828735
Epoch: 6, Batch: 803, Loss: 0.6215135455131531
Epoch: 6, Batch: 804, Loss: 0.4594646990299225
Epoch: 6, Batch: 805, Loss: 0.47863417863845825
Epoch: 6, Batch: 806, Loss: 1.3501724004745483
Epoch: 6, Batch: 807, Loss: 0.5657119154930115
Epoch: 6, Batch: 808, Loss: 0.3727155327796936
Epoch: 6, Batch: 809, Loss: 0.6544972658157349
Epoch: 6, Batch: 810, Loss: 0.3299109637737274
Epoch: 6, Batch: 811, Loss: 0.30915921926498413
Epoch: 6, Batch: 812, Loss: 0.5699189901351929
Epoch: 6, Batch: 813, Loss: 0.2416609823703766
Epoch: 6, Batch: 814, Loss: 0.21395528316497803
Epoch: 6, Batch: 815, Loss: 0.7024718523025513
Epoch: 6, Batch: 816, Loss: 0.28692370653152466
Epoch: 6, Batch: 817, Loss: 0.3347786068916321
Epoch: 6, Batch: 818, Loss: 0.350907564163208
Epoch: 6, Batch: 819, Loss: 0.3887670934200287
Epoch: 6, Batch: 820, Loss: 0.40761086344718933
Epoch: 6, Batch: 821, Loss: 0.29940080642700195
Epoch: 6, Batch: 822, Loss: 0.31175652146339417
Epoch: 6, Batch: 823, Loss: 0.30610793828964233
Epoch: 6, Batch: 824, Loss: 0.513144314289093
Epoch: 6, Batch: 825, Loss: 0.37729397416114807
Epoch: 6, Batch: 826, Loss: 0.3464805781841278
Epoch: 6, Batch: 827, Loss: 0.6500757932662964
Epoch: 6, Batch: 828, Loss: 0.5819291472434998
Epoch: 6, Batch: 829, Loss: 0.31392911076545715
Epoch: 6, Batch: 830, Loss: 0.32525017857551575
Epoch: 6, Batch: 831, Loss: 0.5094824433326721
Epoch: 6, Batch: 832, Loss: 0.238221675157547
Epoch: 6, Batch: 833, Loss: 0.29148203134536743
Epoch: 6, Batch: 834, Loss: 0.3096531629562378
Epoch: 6, Batch: 835, Loss: 0.3288809657096863
Epoch: 6, Batch: 836, Loss: 0.2800354063510895
Epoch: 6, Batch: 837, Loss: 0.3220025599002838
Epoch: 6, Batch: 838, Loss: 0.30527547001838684
Epoch: 6, Batch: 839, Loss: 0.3358156383037567
Epoch: 6, Batch: 840, Loss: 0.307422399520874
Epoch: 6, Batch: 841, Loss: 0.24277283251285553
Epoch: 6, Batch: 842, Loss: 0.5710593461990356
Epoch: 6, Batch: 843, Loss: 0.5087119340896606
Epoch: 6, Batch: 844, Loss: 0.4292888641357422
Epoch: 6, Batch: 845, Loss: 0.5220876932144165
Epoch: 6, Batch: 846, Loss: 0.5536733865737915
Epoch: 6, Batch: 847, Loss: 0.6068034172058105
Epoch: 6, Batch: 848, Loss: 0.45468705892562866
Epoch: 6, Batch: 849, Loss: 0.2680061161518097
Epoch: 6, Batch: 850, Loss: 0.25719091296195984
Epoch: 6, Batch: 851, Loss: 0.36403197050094604
Epoch: 6, Batch: 852, Loss: 0.28659820556640625
Epoch: 6, Batch: 853, Loss: 0.9840686321258545
Epoch: 6, Batch: 854, Loss: 0.21602341532707214
Epoch: 6, Batch: 855, Loss: 0.21669518947601318
Epoch: 6, Batch: 856, Loss: 0.3757774531841278
Epoch: 6, Batch: 857, Loss: 0.2137375771999359
Epoch: 6, Batch: 858, Loss: 0.26000285148620605
Epoch: 6, Batch: 859, Loss: 0.53092360496521
Epoch: 6, Batch: 860, Loss: 0.9036852121353149
Epoch: 6, Batch: 861, Loss: 0.6320719718933105
Epoch: 6, Batch: 862, Loss: 0.259991854429245
Epoch: 6, Batch: 863, Loss: 0.29548364877700806
Epoch: 6, Batch: 864, Loss: 0.31422972679138184
Epoch: 6, Batch: 865, Loss: 0.43096205592155457
Epoch: 6, Batch: 866, Loss: 0.39981573820114136
Epoch: 6, Batch: 867, Loss: 0.39020681381225586
Epoch: 6, Batch: 868, Loss: 0.3838921785354614
Epoch: 6, Batch: 869, Loss: 0.203647643327713
Epoch: 6, Batch: 870, Loss: 0.3965742290019989
Epoch: 6, Batch: 871, Loss: 0.23822525143623352
Epoch: 6, Batch: 872, Loss: 0.29728710651397705
Epoch: 6, Batch: 873, Loss: 0.5413327813148499
Epoch: 6, Batch: 874, Loss: 0.4704250991344452
Epoch: 6, Batch: 875, Loss: 0.4762299954891205
Epoch: 6, Batch: 876, Loss: 0.27506744861602783
Epoch: 6, Batch: 877, Loss: 0.6340938210487366
Epoch: 6, Batch: 878, Loss: 0.3390655219554901
Epoch: 6, Batch: 879, Loss: 0.3380061686038971
Epoch: 6, Batch: 880, Loss: 0.22202245891094208
Epoch: 6, Batch: 881, Loss: 0.24446550011634827
Epoch: 6, Batch: 882, Loss: 0.5663512945175171
Epoch: 6, Batch: 883, Loss: 0.29170387983322144
Epoch: 6, Batch: 884, Loss: 0.3569658696651459
Epoch: 6, Batch: 885, Loss: 0.30313652753829956
Epoch: 6, Batch: 886, Loss: 0.31343862414360046
Epoch: 6, Batch: 887, Loss: 0.2629229724407196
Epoch: 6, Batch: 888, Loss: 0.2562582790851593
Epoch: 6, Batch: 889, Loss: 0.1708279699087143
Epoch: 6, Batch: 890, Loss: 0.1973845362663269
Epoch: 6, Batch: 891, Loss: 0.2300569862127304
Epoch: 6, Batch: 892, Loss: 0.2682838439941406
Epoch: 6, Batch: 893, Loss: 0.2012762576341629
Epoch: 6, Batch: 894, Loss: 0.3216114640235901
Epoch: 6, Batch: 895, Loss: 0.213815838098526
Epoch: 6, Batch: 896, Loss: 0.5319899320602417
Epoch: 6, Batch: 897, Loss: 0.33425742387771606
Epoch: 6, Batch: 898, Loss: 0.47795307636260986
Epoch: 6, Batch: 899, Loss: 0.19934986531734467
Epoch: 6, Batch: 900, Loss: 0.30030301213264465
Epoch: 6, Batch: 901, Loss: 0.6360467672348022
Epoch: 6, Batch: 902, Loss: 0.6672565937042236
Epoch: 6, Batch: 903, Loss: 0.5990189909934998
Epoch: 6, Batch: 904, Loss: 0.43202513456344604
Epoch: 6, Batch: 905, Loss: 0.47087562084198
Epoch: 6, Batch: 906, Loss: 0.3810066878795624
Epoch: 6, Batch: 907, Loss: 0.3436826765537262
Epoch: 6, Batch: 908, Loss: 0.4042550325393677
Epoch: 6, Batch: 909, Loss: 0.2547939717769623
Epoch: 6, Batch: 910, Loss: 0.21365585923194885
Epoch: 6, Batch: 911, Loss: 0.5767343044281006
Epoch: 6, Batch: 912, Loss: 0.36751100420951843
Epoch: 6, Batch: 913, Loss: 0.2569836974143982
Epoch: 6, Batch: 914, Loss: 0.2545166015625
Epoch: 6, Batch: 915, Loss: 0.17577658593654633
Epoch: 6, Batch: 916, Loss: 0.2150905728340149
Epoch: 6, Batch: 917, Loss: 0.20409105718135834
Epoch: 6, Batch: 918, Loss: 0.22164271771907806
Epoch: 6, Batch: 919, Loss: 0.23009081184864044
Epoch: 6, Batch: 920, Loss: 0.6244393587112427
Epoch: 6, Batch: 921, Loss: 0.3668738007545471
Epoch: 6, Batch: 922, Loss: 0.3065166473388672
Epoch: 6, Batch: 923, Loss: 0.6045904159545898
Epoch: 6, Batch: 924, Loss: 0.35238033533096313
Epoch: 6, Batch: 925, Loss: 0.5179229378700256
Epoch: 6, Batch: 926, Loss: 0.41553497314453125
Epoch: 6, Batch: 927, Loss: 0.2503032684326172
Epoch: 6, Batch: 928, Loss: 0.3348892629146576
Epoch: 6, Batch: 929, Loss: 0.5983187556266785
Epoch: 6, Batch: 930, Loss: 0.2969294786453247
Epoch Completed: 6/10, Time: 212.8513593673706, Train Loss: 0.39829815082973047, Valid Loss: 0.3928728569899836

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:15  model_time: 0.3334 (0.3334)  evaluator_time: 0.0052 (0.0052)  time: 0.3762  data: 0.0345  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3308 (0.3276)  evaluator_time: 0.0045 (0.0050)  time: 0.3645  data: 0.0340  max mem: 4996
Test: Total time: 0:00:14 (0.3707 s / it)
Averaged stats: model_time: 0.3308 (0.3276)  evaluator_time: 0.0045 (0.0050)
Accumulating evaluation results...
DONE (t=0.05s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.095
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.007
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.049
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.093
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.193
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.193
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.108
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.196
Epoch: 7, Batch: 931, Loss: 0.6075558066368103
Epoch: 7, Batch: 932, Loss: 0.4597567021846771
Epoch: 7, Batch: 933, Loss: 0.5438517332077026
Epoch: 7, Batch: 934, Loss: 0.35168492794036865
Epoch: 7, Batch: 935, Loss: 0.29358240962028503
Epoch: 7, Batch: 936, Loss: 0.24125131964683533
Epoch: 7, Batch: 937, Loss: 0.5041472315788269
Epoch: 7, Batch: 938, Loss: 0.3011573255062103
Epoch: 7, Batch: 939, Loss: 0.28621217608451843
Epoch: 7, Batch: 940, Loss: 0.27012401819229126
Epoch: 7, Batch: 941, Loss: 0.24323605000972748
Epoch: 7, Batch: 942, Loss: 0.208944633603096
Epoch: 7, Batch: 943, Loss: 0.19762228429317474
Epoch: 7, Batch: 944, Loss: 0.22643986344337463
Epoch: 7, Batch: 945, Loss: 0.32301345467567444
Epoch: 7, Batch: 946, Loss: 0.10790321230888367
Epoch: 7, Batch: 947, Loss: 1.6197466850280762
Epoch: 7, Batch: 948, Loss: 0.22768649458885193
Epoch: 7, Batch: 949, Loss: 0.32817313075065613
Epoch: 7, Batch: 950, Loss: 0.5705360174179077
Epoch: 7, Batch: 951, Loss: 0.45376506447792053
Epoch: 7, Batch: 952, Loss: 0.6449397206306458
Epoch: 7, Batch: 953, Loss: 0.24592448770999908
Epoch: 7, Batch: 954, Loss: 0.4308609068393707
Epoch: 7, Batch: 955, Loss: 0.307889461517334
Epoch: 7, Batch: 956, Loss: 0.27658334374427795
Epoch: 7, Batch: 957, Loss: 0.285334974527359
Epoch: 7, Batch: 958, Loss: 0.5382038354873657
Epoch: 7, Batch: 959, Loss: 0.25199928879737854
Epoch: 7, Batch: 960, Loss: 0.28148576617240906
Epoch: 7, Batch: 961, Loss: 1.9166169166564941
Epoch: 7, Batch: 962, Loss: 0.5360904932022095
Epoch: 7, Batch: 963, Loss: 0.39960262179374695
Epoch: 7, Batch: 964, Loss: 0.7494826316833496
Epoch: 7, Batch: 965, Loss: 0.34001123905181885
Epoch: 7, Batch: 966, Loss: 0.34307777881622314
Epoch: 7, Batch: 967, Loss: 0.5527719259262085
Epoch: 7, Batch: 968, Loss: 0.1931169331073761
Epoch: 7, Batch: 969, Loss: 0.2616446912288666
Epoch: 7, Batch: 970, Loss: 0.819945216178894
Epoch: 7, Batch: 971, Loss: 0.3919624388217926
Epoch: 7, Batch: 972, Loss: 0.41534438729286194
Epoch: 7, Batch: 973, Loss: 0.3455422520637512
Epoch: 7, Batch: 974, Loss: 0.40373194217681885
Epoch: 7, Batch: 975, Loss: 0.4245114028453827
Epoch: 7, Batch: 976, Loss: 0.3735024929046631
Epoch: 7, Batch: 977, Loss: 0.36643826961517334
Epoch: 7, Batch: 978, Loss: 0.3760288655757904
Epoch: 7, Batch: 979, Loss: 0.6409751176834106
Epoch: 7, Batch: 980, Loss: 0.36046749353408813
Epoch: 7, Batch: 981, Loss: 0.4082149863243103
Epoch: 7, Batch: 982, Loss: 0.7975659370422363
Epoch: 7, Batch: 983, Loss: 0.5427772998809814
Epoch: 7, Batch: 984, Loss: 0.34555619955062866
Epoch: 7, Batch: 985, Loss: 0.3275904059410095
Epoch: 7, Batch: 986, Loss: 0.5215870141983032
Epoch: 7, Batch: 987, Loss: 0.28378528356552124
Epoch: 7, Batch: 988, Loss: 0.3032242953777313
Epoch: 7, Batch: 989, Loss: 0.29616421461105347
Epoch: 7, Batch: 990, Loss: 0.3322654068470001
Epoch: 7, Batch: 991, Loss: 0.32480981945991516
Epoch: 7, Batch: 992, Loss: 0.3188764154911041
Epoch: 7, Batch: 993, Loss: 0.32276973128318787
Epoch: 7, Batch: 994, Loss: 0.32747727632522583
Epoch: 7, Batch: 995, Loss: 0.3161165118217468
Epoch: 7, Batch: 996, Loss: 0.2960369288921356
Epoch: 7, Batch: 997, Loss: 0.595474898815155
Epoch: 7, Batch: 998, Loss: 0.5365034341812134
Epoch: 7, Batch: 999, Loss: 0.4177168607711792
Epoch: 7, Batch: 1000, Loss: 0.5825116634368896
Epoch: 7, Batch: 1001, Loss: 0.5069513916969299
Epoch: 7, Batch: 1002, Loss: 0.5539988875389099
Epoch: 7, Batch: 1003, Loss: 0.41737455129623413
Epoch: 7, Batch: 1004, Loss: 0.26155540347099304
Epoch: 7, Batch: 1005, Loss: 0.2823561131954193
Epoch: 7, Batch: 1006, Loss: 0.3620944321155548
Epoch: 7, Batch: 1007, Loss: 0.23723295331001282
Epoch: 7, Batch: 1008, Loss: 0.9437631368637085
Epoch: 7, Batch: 1009, Loss: 0.24042271077632904
Epoch: 7, Batch: 1010, Loss: 0.22020603716373444
Epoch: 7, Batch: 1011, Loss: 0.3876112997531891
Epoch: 7, Batch: 1012, Loss: 0.24390725791454315
Epoch: 7, Batch: 1013, Loss: 0.20692765712738037
Epoch: 7, Batch: 1014, Loss: 0.4655131995677948
Epoch: 7, Batch: 1015, Loss: 0.7765683531761169
Epoch: 7, Batch: 1016, Loss: 0.8384372591972351
Epoch: 7, Batch: 1017, Loss: 0.25720736384391785
Epoch: 7, Batch: 1018, Loss: 0.2841310203075409
Epoch: 7, Batch: 1019, Loss: 0.38690632581710815
Epoch: 7, Batch: 1020, Loss: 0.4014355540275574
Epoch: 7, Batch: 1021, Loss: 0.37416115403175354
Epoch: 7, Batch: 1022, Loss: 0.4013512432575226
Epoch: 7, Batch: 1023, Loss: 0.3812190890312195
Epoch: 7, Batch: 1024, Loss: 0.25166213512420654
Epoch: 7, Batch: 1025, Loss: 0.4797362685203552
Epoch: 7, Batch: 1026, Loss: 0.2677707076072693
Epoch: 7, Batch: 1027, Loss: 0.24455757439136505
Epoch: 7, Batch: 1028, Loss: 0.4713839292526245
Epoch: 7, Batch: 1029, Loss: 0.4321736693382263
Epoch: 7, Batch: 1030, Loss: 0.4158305525779724
Epoch: 7, Batch: 1031, Loss: 0.29658082127571106
Epoch: 7, Batch: 1032, Loss: 0.6254419088363647
Epoch: 7, Batch: 1033, Loss: 0.40776801109313965
Epoch: 7, Batch: 1034, Loss: 0.332550048828125
Epoch: 7, Batch: 1035, Loss: 0.26198211312294006
Epoch: 7, Batch: 1036, Loss: 0.26433634757995605
Epoch: 7, Batch: 1037, Loss: 0.623355507850647
Epoch: 7, Batch: 1038, Loss: 0.3357537090778351
Epoch: 7, Batch: 1039, Loss: 0.3117428421974182
Epoch: 7, Batch: 1040, Loss: 0.2571210563182831
Epoch: 7, Batch: 1041, Loss: 0.24362486600875854
Epoch: 7, Batch: 1042, Loss: 0.25005578994750977
Epoch: 7, Batch: 1043, Loss: 0.2643902003765106
Epoch: 7, Batch: 1044, Loss: 0.16155759990215302
Epoch: 7, Batch: 1045, Loss: 0.22550731897354126
Epoch: 7, Batch: 1046, Loss: 0.25068315863609314
Epoch: 7, Batch: 1047, Loss: 0.2574731111526489
Epoch: 7, Batch: 1048, Loss: 0.1834045946598053
Epoch: 7, Batch: 1049, Loss: 0.2729814350605011
Epoch: 7, Batch: 1050, Loss: 0.24405401945114136
Epoch: 7, Batch: 1051, Loss: 0.5292961001396179
Epoch: 7, Batch: 1052, Loss: 0.30525824427604675
Epoch: 7, Batch: 1053, Loss: 0.4630632698535919
Epoch: 7, Batch: 1054, Loss: 0.21664020419120789
Epoch: 7, Batch: 1055, Loss: 0.4107356071472168
Epoch: 7, Batch: 1056, Loss: 0.6285268068313599
Epoch: 7, Batch: 1057, Loss: 0.5773280262947083
Epoch: 7, Batch: 1058, Loss: 0.7556632161140442
Epoch: 7, Batch: 1059, Loss: 0.46291595697402954
Epoch: 7, Batch: 1060, Loss: 0.5501716136932373
Epoch: 7, Batch: 1061, Loss: 0.4353514313697815
Epoch: 7, Batch: 1062, Loss: 0.3030396103858948
Epoch: 7, Batch: 1063, Loss: 0.4253741502761841
Epoch: 7, Batch: 1064, Loss: 0.2577444911003113
Epoch: 7, Batch: 1065, Loss: 0.20613813400268555
Epoch: 7, Batch: 1066, Loss: 0.5374768376350403
Epoch: 7, Batch: 1067, Loss: 0.3024519681930542
Epoch: 7, Batch: 1068, Loss: 0.21031223237514496
Epoch: 7, Batch: 1069, Loss: 0.25346076488494873
Epoch: 7, Batch: 1070, Loss: 0.15031775832176208
Epoch: 7, Batch: 1071, Loss: 0.2150002270936966
Epoch: 7, Batch: 1072, Loss: 0.18798264861106873
Epoch: 7, Batch: 1073, Loss: 0.2245340347290039
Epoch: 7, Batch: 1074, Loss: 0.2565036118030548
Epoch: 7, Batch: 1075, Loss: 0.7067116498947144
Epoch: 7, Batch: 1076, Loss: 0.38928788900375366
Epoch: 7, Batch: 1077, Loss: 0.31831008195877075
Epoch: 7, Batch: 1078, Loss: 0.5666600465774536
Epoch: 7, Batch: 1079, Loss: 0.2849554717540741
Epoch: 7, Batch: 1080, Loss: 0.4140566289424896
Epoch: 7, Batch: 1081, Loss: 0.3551194667816162
Epoch: 7, Batch: 1082, Loss: 0.26609429717063904
Epoch: 7, Batch: 1083, Loss: 0.40865063667297363
Epoch: 7, Batch: 1084, Loss: 0.5589169859886169
Epoch: 7, Batch: 1085, Loss: 0.4546066224575043
Epoch Completed: 7/10, Time: 212.96520113945007, Train Loss: 0.39800626087573265, Valid Loss: 0.39769077127979646

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:14  model_time: 0.3189 (0.3189)  evaluator_time: 0.0055 (0.0055)  time: 0.3619  data: 0.0344  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3279 (0.3236)  evaluator_time: 0.0046 (0.0061)  time: 0.3625  data: 0.0338  max mem: 4996
Test: Total time: 0:00:14 (0.3679 s / it)
Averaged stats: model_time: 0.3279 (0.3236)  evaluator_time: 0.0046 (0.0061)
Accumulating evaluation results...
DONE (t=0.06s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.018
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.051
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.042
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.070
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.167
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.170
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.172
Epoch: 8, Batch: 1086, Loss: 0.5939329862594604
Epoch: 8, Batch: 1087, Loss: 0.5368531942367554
Epoch: 8, Batch: 1088, Loss: 0.6933008432388306
Epoch: 8, Batch: 1089, Loss: 0.36346864700317383
Epoch: 8, Batch: 1090, Loss: 0.2260511815547943
Epoch: 8, Batch: 1091, Loss: 0.26854103803634644
Epoch: 8, Batch: 1092, Loss: 0.5100753307342529
Epoch: 8, Batch: 1093, Loss: 0.2319900244474411
Epoch: 8, Batch: 1094, Loss: 0.3535860478878021
Epoch: 8, Batch: 1095, Loss: 0.2829963266849518
Epoch: 8, Batch: 1096, Loss: 0.22819216549396515
Epoch: 8, Batch: 1097, Loss: 0.20318958163261414
Epoch: 8, Batch: 1098, Loss: 0.1867009699344635
Epoch: 8, Batch: 1099, Loss: 0.27539923787117004
Epoch: 8, Batch: 1100, Loss: 0.31161054968833923
Epoch: 8, Batch: 1101, Loss: 0.10019863396883011
Epoch: 8, Batch: 1102, Loss: 1.640267252922058
Epoch: 8, Batch: 1103, Loss: 0.22857829928398132
Epoch: 8, Batch: 1104, Loss: 0.28605371713638306
Epoch: 8, Batch: 1105, Loss: 0.6339672803878784
Epoch: 8, Batch: 1106, Loss: 0.5016570091247559
Epoch: 8, Batch: 1107, Loss: 0.780349850654602
Epoch: 8, Batch: 1108, Loss: 0.2546767592430115
Epoch: 8, Batch: 1109, Loss: 0.4174397885799408
Epoch: 8, Batch: 1110, Loss: 0.3349931836128235
Epoch: 8, Batch: 1111, Loss: 0.26128530502319336
Epoch: 8, Batch: 1112, Loss: 0.3323380947113037
Epoch: 8, Batch: 1113, Loss: 0.4819721281528473
Epoch: 8, Batch: 1114, Loss: 0.3115769624710083
Epoch: 8, Batch: 1115, Loss: 0.32293209433555603
Epoch: 8, Batch: 1116, Loss: 1.7050981521606445
Epoch: 8, Batch: 1117, Loss: 0.5118868350982666
Epoch: 8, Batch: 1118, Loss: 0.37981802225112915
Epoch: 8, Batch: 1119, Loss: 0.684324324131012
Epoch: 8, Batch: 1120, Loss: 0.31491050124168396
Epoch: 8, Batch: 1121, Loss: 0.3780645430088043
Epoch: 8, Batch: 1122, Loss: 0.602512776851654
Epoch: 8, Batch: 1123, Loss: 0.18996378779411316
Epoch: 8, Batch: 1124, Loss: 0.21516253054141998
Epoch: 8, Batch: 1125, Loss: 0.5885728001594543
Epoch: 8, Batch: 1126, Loss: 0.34367281198501587
Epoch: 8, Batch: 1127, Loss: 0.3669285178184509
Epoch: 8, Batch: 1128, Loss: 0.28068116307258606
Epoch: 8, Batch: 1129, Loss: 0.34231799840927124
Epoch: 8, Batch: 1130, Loss: 0.35902345180511475
Epoch: 8, Batch: 1131, Loss: 0.26952412724494934
Epoch: 8, Batch: 1132, Loss: 0.2729020416736603
Epoch: 8, Batch: 1133, Loss: 0.34518229961395264
Epoch: 8, Batch: 1134, Loss: 0.6149352192878723
Epoch: 8, Batch: 1135, Loss: 0.3711327314376831
Epoch: 8, Batch: 1136, Loss: 0.38648971915245056
Epoch: 8, Batch: 1137, Loss: 0.7208874821662903
Epoch: 8, Batch: 1138, Loss: 0.4788571894168854
Epoch: 8, Batch: 1139, Loss: 0.2930440604686737
Epoch: 8, Batch: 1140, Loss: 0.28234735131263733
Epoch: 8, Batch: 1141, Loss: 0.4451937675476074
Epoch: 8, Batch: 1142, Loss: 0.22571496665477753
Epoch: 8, Batch: 1143, Loss: 0.3771853446960449
Epoch: 8, Batch: 1144, Loss: 0.2881862223148346
Epoch: 8, Batch: 1145, Loss: 0.27894866466522217
Epoch: 8, Batch: 1146, Loss: 0.28280431032180786
Epoch: 8, Batch: 1147, Loss: 0.3392668664455414
Epoch: 8, Batch: 1148, Loss: 0.4031314253807068
Epoch: 8, Batch: 1149, Loss: 0.2980015277862549
Epoch: 8, Batch: 1150, Loss: 0.29403209686279297
Epoch: 8, Batch: 1151, Loss: 0.23614895343780518
Epoch: 8, Batch: 1152, Loss: 0.562244176864624
Epoch: 8, Batch: 1153, Loss: 0.42012980580329895
Epoch: 8, Batch: 1154, Loss: 0.378366619348526
Epoch: 8, Batch: 1155, Loss: 0.4664513170719147
Epoch: 8, Batch: 1156, Loss: 0.4298133850097656
Epoch: 8, Batch: 1157, Loss: 0.5064148306846619
Epoch: 8, Batch: 1158, Loss: 0.3715667724609375
Epoch: 8, Batch: 1159, Loss: 0.2726984918117523
Epoch: 8, Batch: 1160, Loss: 0.22939614951610565
Epoch: 8, Batch: 1161, Loss: 0.33598804473876953
Epoch: 8, Batch: 1162, Loss: 0.20486055314540863
Epoch: 8, Batch: 1163, Loss: 1.1622998714447021
Epoch: 8, Batch: 1164, Loss: 0.23854829370975494
Epoch: 8, Batch: 1165, Loss: 0.222930446267128
Epoch: 8, Batch: 1166, Loss: 0.2938656806945801
Epoch: 8, Batch: 1167, Loss: 0.21265749633312225
Epoch: 8, Batch: 1168, Loss: 0.17322033643722534
Epoch: 8, Batch: 1169, Loss: 0.37341582775115967
Epoch: 8, Batch: 1170, Loss: 0.45836395025253296
Epoch: 8, Batch: 1171, Loss: 0.5514074563980103
Epoch: 8, Batch: 1172, Loss: 0.2557586431503296
Epoch: 8, Batch: 1173, Loss: 0.2988073527812958
Epoch: 8, Batch: 1174, Loss: 0.27756038308143616
Epoch: 8, Batch: 1175, Loss: 0.44252830743789673
Epoch: 8, Batch: 1176, Loss: 0.30014461278915405
Epoch: 8, Batch: 1177, Loss: 0.3936229646205902
Epoch: 8, Batch: 1178, Loss: 0.37135082483291626
Epoch: 8, Batch: 1179, Loss: 0.17634104192256927
Epoch: 8, Batch: 1180, Loss: 0.35275569558143616
Epoch: 8, Batch: 1181, Loss: 0.2555233836174011
Epoch: 8, Batch: 1182, Loss: 0.24992360174655914
Epoch: 8, Batch: 1183, Loss: 0.6167631149291992
Epoch: 8, Batch: 1184, Loss: 0.4316391944885254
Epoch: 8, Batch: 1185, Loss: 0.4386245608329773
Epoch: 8, Batch: 1186, Loss: 0.3024309277534485
Epoch: 8, Batch: 1187, Loss: 0.5389959216117859
Epoch: 8, Batch: 1188, Loss: 0.3332468569278717
Epoch: 8, Batch: 1189, Loss: 0.29566872119903564
Epoch: 8, Batch: 1190, Loss: 0.22664238512516022
Epoch: 8, Batch: 1191, Loss: 0.21629032492637634
Epoch: 8, Batch: 1192, Loss: 0.4945647120475769
Epoch: 8, Batch: 1193, Loss: 0.3220519423484802
Epoch: 8, Batch: 1194, Loss: 0.2778523862361908
Epoch: 8, Batch: 1195, Loss: 0.21177294850349426
Epoch: 8, Batch: 1196, Loss: 0.21035490930080414
Epoch: 8, Batch: 1197, Loss: 0.1952689290046692
Epoch: 8, Batch: 1198, Loss: 0.22631701827049255
Epoch: 8, Batch: 1199, Loss: 0.13836432993412018
Epoch: 8, Batch: 1200, Loss: 0.19022852182388306
Epoch: 8, Batch: 1201, Loss: 0.1847658008337021
Epoch: 8, Batch: 1202, Loss: 0.23491045832633972
Epoch: 8, Batch: 1203, Loss: 0.16384483873844147
Epoch: 8, Batch: 1204, Loss: 0.20819208025932312
Epoch: 8, Batch: 1205, Loss: 0.19347989559173584
Epoch: 8, Batch: 1206, Loss: 0.45889097452163696
Epoch: 8, Batch: 1207, Loss: 0.35305020213127136
Epoch: 8, Batch: 1208, Loss: 0.44360920786857605
Epoch: 8, Batch: 1209, Loss: 0.2489769607782364
Epoch: 8, Batch: 1210, Loss: 0.3678332269191742
Epoch: 8, Batch: 1211, Loss: 0.4626524746417999
Epoch: 8, Batch: 1212, Loss: 0.5512959957122803
Epoch: 8, Batch: 1213, Loss: 0.6576444506645203
Epoch: 8, Batch: 1214, Loss: 0.38961517810821533
Epoch: 8, Batch: 1215, Loss: 0.41656291484832764
Epoch: 8, Batch: 1216, Loss: 0.34790539741516113
Epoch: 8, Batch: 1217, Loss: 0.2815018594264984
Epoch: 8, Batch: 1218, Loss: 0.4218100905418396
Epoch: 8, Batch: 1219, Loss: 0.2944068908691406
Epoch: 8, Batch: 1220, Loss: 0.2066735029220581
Epoch: 8, Batch: 1221, Loss: 0.46810364723205566
Epoch: 8, Batch: 1222, Loss: 0.28141042590141296
Epoch: 8, Batch: 1223, Loss: 0.17747189104557037
Epoch: 8, Batch: 1224, Loss: 0.22480379045009613
Epoch: 8, Batch: 1225, Loss: 0.15296676754951477
Epoch: 8, Batch: 1226, Loss: 0.19750253856182098
Epoch: 8, Batch: 1227, Loss: 0.18932399153709412
Epoch: 8, Batch: 1228, Loss: 0.16914242506027222
Epoch: 8, Batch: 1229, Loss: 0.2998354434967041
Epoch: 8, Batch: 1230, Loss: 0.36343252658843994
Epoch: 8, Batch: 1231, Loss: 0.3405075669288635
Epoch: 8, Batch: 1232, Loss: 0.26908397674560547
Epoch: 8, Batch: 1233, Loss: 0.4943128228187561
Epoch: 8, Batch: 1234, Loss: 0.3080536723136902
Epoch: 8, Batch: 1235, Loss: 0.4003017842769623
Epoch: 8, Batch: 1236, Loss: 0.3701072335243225
Epoch: 8, Batch: 1237, Loss: 0.30505549907684326
Epoch: 8, Batch: 1238, Loss: 0.3201681971549988
Epoch: 8, Batch: 1239, Loss: 0.6010003089904785
Epoch: 8, Batch: 1240, Loss: 0.4318521320819855
Epoch Completed: 8/10, Time: 212.97300505638123, Train Loss: 0.3682524008616324, Valid Loss: 0.3704250676016654

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:14  model_time: 0.3326 (0.3326)  evaluator_time: 0.0033 (0.0033)  time: 0.3734  data: 0.0343  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3270 (0.3235)  evaluator_time: 0.0041 (0.0041)  time: 0.3606  data: 0.0341  max mem: 4996
Test: Total time: 0:00:14 (0.3657 s / it)
Averaged stats: model_time: 0.3270 (0.3235)  evaluator_time: 0.0041 (0.0041)
Accumulating evaluation results...
DONE (t=0.04s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.053
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.023
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.050
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.076
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.144
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.125
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.145
Epoch: 9, Batch: 1241, Loss: 0.6237695217132568
Epoch: 9, Batch: 1242, Loss: 0.4051945209503174
Epoch: 9, Batch: 1243, Loss: 0.4620514512062073
Epoch: 9, Batch: 1244, Loss: 0.2847314476966858
Epoch: 9, Batch: 1245, Loss: 0.25108158588409424
Epoch: 9, Batch: 1246, Loss: 0.19440127909183502
Epoch: 9, Batch: 1247, Loss: 0.5706267952919006
Epoch: 9, Batch: 1248, Loss: 0.26847660541534424
Epoch: 9, Batch: 1249, Loss: 0.3316972553730011
Epoch: 9, Batch: 1250, Loss: 0.279666930437088
Epoch: 9, Batch: 1251, Loss: 0.2824834883213043
Epoch: 9, Batch: 1252, Loss: 0.21613414585590363
Epoch: 9, Batch: 1253, Loss: 0.17998364567756653
Epoch: 9, Batch: 1254, Loss: 0.2793930768966675
Epoch: 9, Batch: 1255, Loss: 0.3326888084411621
Epoch: 9, Batch: 1256, Loss: 0.15665403008460999
Epoch: 9, Batch: 1257, Loss: 1.4395482540130615
Epoch: 9, Batch: 1258, Loss: 0.22877661883831024
Epoch: 9, Batch: 1259, Loss: 0.26836079359054565
Epoch: 9, Batch: 1260, Loss: 0.6841956377029419
Epoch: 9, Batch: 1261, Loss: 0.3896220028400421
Epoch: 9, Batch: 1262, Loss: 0.8719133734703064
Epoch: 9, Batch: 1263, Loss: 0.22200842201709747
Epoch: 9, Batch: 1264, Loss: 0.38371989130973816
Epoch: 9, Batch: 1265, Loss: 0.2894688844680786
Epoch: 9, Batch: 1266, Loss: 0.21599411964416504
Epoch: 9, Batch: 1267, Loss: 0.24559272825717926
Epoch: 9, Batch: 1268, Loss: 0.5028340816497803
Epoch: 9, Batch: 1269, Loss: 0.48505866527557373
Epoch: 9, Batch: 1270, Loss: 0.4996621608734131
Epoch: 9, Batch: 1271, Loss: 2.000741720199585
Epoch: 9, Batch: 1272, Loss: 0.5689570307731628
Epoch: 9, Batch: 1273, Loss: 0.3739992380142212
Epoch: 9, Batch: 1274, Loss: 0.6838890314102173
Epoch: 9, Batch: 1275, Loss: 0.24690783023834229
Epoch: 9, Batch: 1276, Loss: 0.24208688735961914
Epoch: 9, Batch: 1277, Loss: 0.45891904830932617
Epoch: 9, Batch: 1278, Loss: 0.20303286612033844
Epoch: 9, Batch: 1279, Loss: 0.17969965934753418
Epoch: 9, Batch: 1280, Loss: 0.8168287873268127
Epoch: 9, Batch: 1281, Loss: 0.28518450260162354
Epoch: 9, Batch: 1282, Loss: 0.3308570683002472
Epoch: 9, Batch: 1283, Loss: 0.2880450487136841
Epoch: 9, Batch: 1284, Loss: 0.3175373673439026
Epoch: 9, Batch: 1285, Loss: 0.32046300172805786
Epoch: 9, Batch: 1286, Loss: 0.24556025862693787
Epoch: 9, Batch: 1287, Loss: 0.29057493805885315
Epoch: 9, Batch: 1288, Loss: 0.28631988167762756
Epoch: 9, Batch: 1289, Loss: 0.5183007717132568
Epoch: 9, Batch: 1290, Loss: 0.29351934790611267
Epoch: 9, Batch: 1291, Loss: 0.2533593475818634
Epoch: 9, Batch: 1292, Loss: 0.7760564088821411
Epoch: 9, Batch: 1293, Loss: 0.45949500799179077
Epoch: 9, Batch: 1294, Loss: 0.2928396463394165
Epoch: 9, Batch: 1295, Loss: 0.32773879170417786
Epoch: 9, Batch: 1296, Loss: 0.4346885681152344
Epoch: 9, Batch: 1297, Loss: 0.31902697682380676
Epoch: 9, Batch: 1298, Loss: 0.30019980669021606
Epoch: 9, Batch: 1299, Loss: 0.2742989659309387
Epoch: 9, Batch: 1300, Loss: 0.327517032623291
Epoch: 9, Batch: 1301, Loss: 0.28694799542427063
Epoch: 9, Batch: 1302, Loss: 0.3317830562591553
Epoch: 9, Batch: 1303, Loss: 0.3746795952320099
Epoch: 9, Batch: 1304, Loss: 0.30469876527786255
Epoch: 9, Batch: 1305, Loss: 0.29868051409721375
Epoch: 9, Batch: 1306, Loss: 0.25424134731292725
Epoch: 9, Batch: 1307, Loss: 0.5479003190994263
Epoch: 9, Batch: 1308, Loss: 0.4916455149650574
Epoch: 9, Batch: 1309, Loss: 0.35045263171195984
Epoch: 9, Batch: 1310, Loss: 0.42848101258277893
Epoch: 9, Batch: 1311, Loss: 0.45699048042297363
Epoch: 9, Batch: 1312, Loss: 0.5225354433059692
Epoch: 9, Batch: 1313, Loss: 0.37411031126976013
Epoch: 9, Batch: 1314, Loss: 0.26490524411201477
Epoch: 9, Batch: 1315, Loss: 0.2215559035539627
Epoch: 9, Batch: 1316, Loss: 0.29075533151626587
Epoch: 9, Batch: 1317, Loss: 0.23039954900741577
Epoch: 9, Batch: 1318, Loss: 0.9637094736099243
Epoch: 9, Batch: 1319, Loss: 0.1900460124015808
Epoch: 9, Batch: 1320, Loss: 0.16961237788200378
Epoch: 9, Batch: 1321, Loss: 0.3371405601501465
Epoch: 9, Batch: 1322, Loss: 0.19864700734615326
Epoch: 9, Batch: 1323, Loss: 0.24815478920936584
Epoch: 9, Batch: 1324, Loss: 0.4103662073612213
Epoch: 9, Batch: 1325, Loss: 0.4313126802444458
Epoch: 9, Batch: 1326, Loss: 0.6941648721694946
Epoch: 9, Batch: 1327, Loss: 0.24704253673553467
Epoch: 9, Batch: 1328, Loss: 0.32525619864463806
Epoch: 9, Batch: 1329, Loss: 0.31100156903266907
Epoch: 9, Batch: 1330, Loss: 0.375522255897522
Epoch: 9, Batch: 1331, Loss: 0.283740758895874
Epoch: 9, Batch: 1332, Loss: 0.3084438741207123
Epoch: 9, Batch: 1333, Loss: 0.3279617428779602
Epoch: 9, Batch: 1334, Loss: 0.1889737844467163
Epoch: 9, Batch: 1335, Loss: 0.4528478682041168
Epoch: 9, Batch: 1336, Loss: 0.20148319005966187
Epoch: 9, Batch: 1337, Loss: 0.19514313340187073
Epoch: 9, Batch: 1338, Loss: 0.4232295751571655
Epoch: 9, Batch: 1339, Loss: 0.4554615914821625
Epoch: 9, Batch: 1340, Loss: 0.3548181354999542
Epoch: 9, Batch: 1341, Loss: 0.2777821719646454
Epoch: 9, Batch: 1342, Loss: 0.6305004954338074
Epoch: 9, Batch: 1343, Loss: 0.33304518461227417
Epoch: 9, Batch: 1344, Loss: 0.31300097703933716
Epoch: 9, Batch: 1345, Loss: 0.19613121449947357
Epoch: 9, Batch: 1346, Loss: 0.21393294632434845
Epoch: 9, Batch: 1347, Loss: 0.44156163930892944
Epoch: 9, Batch: 1348, Loss: 0.2758933901786804
Epoch: 9, Batch: 1349, Loss: 0.35877856612205505
Epoch: 9, Batch: 1350, Loss: 0.2030598223209381
Epoch: 9, Batch: 1351, Loss: 0.26860010623931885
Epoch: 9, Batch: 1352, Loss: 0.25035029649734497
Epoch: 9, Batch: 1353, Loss: 0.23452644050121307
Epoch: 9, Batch: 1354, Loss: 0.16588684916496277
Epoch: 9, Batch: 1355, Loss: 0.17473620176315308
Epoch: 9, Batch: 1356, Loss: 0.19102022051811218
Epoch: 9, Batch: 1357, Loss: 0.22630010545253754
Epoch: 9, Batch: 1358, Loss: 0.15322092175483704
Epoch: 9, Batch: 1359, Loss: 0.22531934082508087
Epoch: 9, Batch: 1360, Loss: 0.2095210999250412
Epoch: 9, Batch: 1361, Loss: 0.46281224489212036
Epoch: 9, Batch: 1362, Loss: 0.2843118906021118
Epoch: 9, Batch: 1363, Loss: 0.4247231185436249
Epoch: 9, Batch: 1364, Loss: 0.21933849155902863
Epoch: 9, Batch: 1365, Loss: 0.3055950701236725
Epoch: 9, Batch: 1366, Loss: 0.5124449729919434
Epoch: 9, Batch: 1367, Loss: 0.4775424897670746
Epoch: 9, Batch: 1368, Loss: 0.60414719581604
Epoch: 9, Batch: 1369, Loss: 0.3877614140510559
Epoch: 9, Batch: 1370, Loss: 0.3991965055465698
Epoch: 9, Batch: 1371, Loss: 0.3011496067047119
Epoch: 9, Batch: 1372, Loss: 0.1858510673046112
Epoch: 9, Batch: 1373, Loss: 0.3362172842025757
Epoch: 9, Batch: 1374, Loss: 0.2944396138191223
Epoch: 9, Batch: 1375, Loss: 0.1582859754562378
Epoch: 9, Batch: 1376, Loss: 0.4025862216949463
Epoch: 9, Batch: 1377, Loss: 0.26543503999710083
Epoch: 9, Batch: 1378, Loss: 0.225228413939476
Epoch: 9, Batch: 1379, Loss: 0.28357967734336853
Epoch: 9, Batch: 1380, Loss: 0.14113692939281464
Epoch: 9, Batch: 1381, Loss: 0.19992244243621826
Epoch: 9, Batch: 1382, Loss: 0.1785624623298645
Epoch: 9, Batch: 1383, Loss: 0.1818738877773285
Epoch: 9, Batch: 1384, Loss: 0.18820543587207794
Epoch: 9, Batch: 1385, Loss: 0.36153292655944824
Epoch: 9, Batch: 1386, Loss: 0.37950295209884644
Epoch: 9, Batch: 1387, Loss: 0.2773725986480713
Epoch: 9, Batch: 1388, Loss: 0.5711812973022461
Epoch: 9, Batch: 1389, Loss: 0.22219817340373993
Epoch: 9, Batch: 1390, Loss: 0.41411975026130676
Epoch: 9, Batch: 1391, Loss: 0.3564085364341736
Epoch: 9, Batch: 1392, Loss: 0.3095734119415283
Epoch: 9, Batch: 1393, Loss: 0.38843709230422974
Epoch: 9, Batch: 1394, Loss: 0.6960737705230713
Epoch: 9, Batch: 1395, Loss: 0.31484490633010864
Epoch Completed: 9/10, Time: 212.82411170005798, Train Loss: 0.35962605889766447, Valid Loss: 0.3767694704955624

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:14  model_time: 0.3304 (0.3304)  evaluator_time: 0.0056 (0.0056)  time: 0.3739  data: 0.0347  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3264 (0.3230)  evaluator_time: 0.0045 (0.0057)  time: 0.3600  data: 0.0338  max mem: 4996
Test: Total time: 0:00:14 (0.3668 s / it)
Averaged stats: model_time: 0.3264 (0.3230)  evaluator_time: 0.0045 (0.0057)
Accumulating evaluation results...
DONE (t=0.05s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.066
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.013
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.002
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.053
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.113
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.229
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.232
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.183
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.235
Epoch: 10, Batch: 1396, Loss: 0.5047796368598938
Epoch: 10, Batch: 1397, Loss: 0.39155855774879456
Epoch: 10, Batch: 1398, Loss: 0.5415840148925781
Epoch: 10, Batch: 1399, Loss: 0.3926225006580353
Epoch: 10, Batch: 1400, Loss: 0.32051950693130493
Epoch: 10, Batch: 1401, Loss: 0.24278220534324646
Epoch: 10, Batch: 1402, Loss: 0.5024604797363281
Epoch: 10, Batch: 1403, Loss: 0.2677803933620453
Epoch: 10, Batch: 1404, Loss: 0.27800098061561584
Epoch: 10, Batch: 1405, Loss: 0.3245190382003784
Epoch: 10, Batch: 1406, Loss: 0.22600895166397095
Epoch: 10, Batch: 1407, Loss: 0.20515099167823792
Epoch: 10, Batch: 1408, Loss: 0.17258846759796143
Epoch: 10, Batch: 1409, Loss: 0.25300753116607666
Epoch: 10, Batch: 1410, Loss: 0.2684340476989746
Epoch: 10, Batch: 1411, Loss: 0.1272779107093811
Epoch: 10, Batch: 1412, Loss: 1.7125409841537476
Epoch: 10, Batch: 1413, Loss: 0.24152567982673645
Epoch: 10, Batch: 1414, Loss: 0.28937217593193054
Epoch: 10, Batch: 1415, Loss: 0.7619402408599854
Epoch: 10, Batch: 1416, Loss: 0.5758734941482544
Epoch: 10, Batch: 1417, Loss: 0.7310473918914795
Epoch: 10, Batch: 1418, Loss: 0.2888137102127075
Epoch: 10, Batch: 1419, Loss: 0.35111474990844727
Epoch: 10, Batch: 1420, Loss: 0.32403719425201416
Epoch: 10, Batch: 1421, Loss: 0.23647087812423706
Epoch: 10, Batch: 1422, Loss: 0.30714094638824463
Epoch: 10, Batch: 1423, Loss: 0.5276169776916504
Epoch: 10, Batch: 1424, Loss: 0.3313223719596863
Epoch: 10, Batch: 1425, Loss: 0.3352160155773163
Epoch: 10, Batch: 1426, Loss: 1.346177101135254
Epoch: 10, Batch: 1427, Loss: 0.44343626499176025
Epoch: 10, Batch: 1428, Loss: 0.2740180194377899
Epoch: 10, Batch: 1429, Loss: 0.7162026166915894
Epoch: 10, Batch: 1430, Loss: 0.29134899377822876
Epoch: 10, Batch: 1431, Loss: 0.2685558497905731
Epoch: 10, Batch: 1432, Loss: 0.40118515491485596
Epoch: 10, Batch: 1433, Loss: 0.1546013057231903
Epoch: 10, Batch: 1434, Loss: 0.16229529678821564
Epoch: 10, Batch: 1435, Loss: 0.5074287056922913
Epoch: 10, Batch: 1436, Loss: 0.28268274664878845
Epoch: 10, Batch: 1437, Loss: 0.36922508478164673
Epoch: 10, Batch: 1438, Loss: 0.2630976438522339
Epoch: 10, Batch: 1439, Loss: 0.3118647038936615
Epoch: 10, Batch: 1440, Loss: 0.320734441280365
Epoch: 10, Batch: 1441, Loss: 0.23828476667404175
Epoch: 10, Batch: 1442, Loss: 0.26669448614120483
Epoch: 10, Batch: 1443, Loss: 0.21458052098751068
Epoch: 10, Batch: 1444, Loss: 0.4681001603603363
Epoch: 10, Batch: 1445, Loss: 0.2697359323501587
Epoch: 10, Batch: 1446, Loss: 0.237938791513443
Epoch: 10, Batch: 1447, Loss: 0.6267455816268921
Epoch: 10, Batch: 1448, Loss: 0.36949422955513
Epoch: 10, Batch: 1449, Loss: 0.2615751028060913
Epoch: 10, Batch: 1450, Loss: 0.29447248578071594
Epoch: 10, Batch: 1451, Loss: 0.38551202416419983
Epoch: 10, Batch: 1452, Loss: 0.22743888199329376
Epoch: 10, Batch: 1453, Loss: 0.24263547360897064
Epoch: 10, Batch: 1454, Loss: 0.23525677621364594
Epoch: 10, Batch: 1455, Loss: 0.2389240264892578
Epoch: 10, Batch: 1456, Loss: 0.25791120529174805
Epoch: 10, Batch: 1457, Loss: 0.30444231629371643
Epoch: 10, Batch: 1458, Loss: 0.3260647654533386
Epoch: 10, Batch: 1459, Loss: 0.32092979550361633
Epoch: 10, Batch: 1460, Loss: 0.2897970974445343
Epoch: 10, Batch: 1461, Loss: 0.276530921459198
Epoch: 10, Batch: 1462, Loss: 0.5514503717422485
Epoch: 10, Batch: 1463, Loss: 0.38024061918258667
Epoch: 10, Batch: 1464, Loss: 0.38951629400253296
Epoch: 10, Batch: 1465, Loss: 0.4828011393547058
Epoch: 10, Batch: 1466, Loss: 0.413445383310318
Epoch: 10, Batch: 1467, Loss: 0.42751172184944153
Epoch: 10, Batch: 1468, Loss: 0.31893616914749146
Epoch: 10, Batch: 1469, Loss: 0.23942257463932037
Epoch: 10, Batch: 1470, Loss: 0.19634532928466797
Epoch: 10, Batch: 1471, Loss: 0.3034662902355194
Epoch: 10, Batch: 1472, Loss: 0.19288405776023865
Epoch: 10, Batch: 1473, Loss: 0.948172926902771
Epoch: 10, Batch: 1474, Loss: 0.18351589143276215
Epoch: 10, Batch: 1475, Loss: 0.1970328986644745
Epoch: 10, Batch: 1476, Loss: 0.2487320899963379
Epoch: 10, Batch: 1477, Loss: 0.16724632680416107
Epoch: 10, Batch: 1478, Loss: 0.1992475688457489
Epoch: 10, Batch: 1479, Loss: 0.3635810911655426
Epoch: 10, Batch: 1480, Loss: 0.6246362328529358
Epoch: 10, Batch: 1481, Loss: 0.5967840552330017
Epoch: 10, Batch: 1482, Loss: 0.22104224562644958
Epoch: 10, Batch: 1483, Loss: 0.30768483877182007
Epoch: 10, Batch: 1484, Loss: 0.2615819573402405
Epoch: 10, Batch: 1485, Loss: 0.30870264768600464
Epoch: 10, Batch: 1486, Loss: 0.27960193157196045
Epoch: 10, Batch: 1487, Loss: 0.2897785007953644
Epoch: 10, Batch: 1488, Loss: 0.33478569984436035
Epoch: 10, Batch: 1489, Loss: 0.1823684573173523
Epoch: 10, Batch: 1490, Loss: 0.4024105966091156
Epoch: 10, Batch: 1491, Loss: 0.18365277349948883
Epoch: 10, Batch: 1492, Loss: 0.17483261227607727
Epoch: 10, Batch: 1493, Loss: 0.5201728343963623
Epoch: 10, Batch: 1494, Loss: 0.42582976818084717
Epoch: 10, Batch: 1495, Loss: 0.34229397773742676
Epoch: 10, Batch: 1496, Loss: 0.24579185247421265
Epoch: 10, Batch: 1497, Loss: 0.4848291873931885
Epoch: 10, Batch: 1498, Loss: 0.3268401622772217
Epoch: 10, Batch: 1499, Loss: 0.30230382084846497
Epoch: 10, Batch: 1500, Loss: 0.1723463386297226
Epoch: 10, Batch: 1501, Loss: 0.21701599657535553
Epoch: 10, Batch: 1502, Loss: 0.37369096279144287
Epoch: 10, Batch: 1503, Loss: 0.23906707763671875
Epoch: 10, Batch: 1504, Loss: 0.3179837763309479
Epoch: 10, Batch: 1505, Loss: 0.2230069488286972
Epoch: 10, Batch: 1506, Loss: 0.2245391607284546
Epoch: 10, Batch: 1507, Loss: 0.2185240089893341
Epoch: 10, Batch: 1508, Loss: 0.1995370239019394
Epoch: 10, Batch: 1509, Loss: 0.12226083129644394
Epoch: 10, Batch: 1510, Loss: 0.17693738639354706
Epoch: 10, Batch: 1511, Loss: 0.22592245042324066
Epoch: 10, Batch: 1512, Loss: 0.2260672003030777
Epoch: 10, Batch: 1513, Loss: 0.14107322692871094
Epoch: 10, Batch: 1514, Loss: 0.22337737679481506
Epoch: 10, Batch: 1515, Loss: 0.15498273074626923
Epoch: 10, Batch: 1516, Loss: 0.40520739555358887
Epoch: 10, Batch: 1517, Loss: 0.3143005669116974
Epoch: 10, Batch: 1518, Loss: 0.41552847623825073
Epoch: 10, Batch: 1519, Loss: 0.23116081953048706
Epoch: 10, Batch: 1520, Loss: 0.3203671872615814
Epoch: 10, Batch: 1521, Loss: 0.486566960811615
Epoch: 10, Batch: 1522, Loss: 0.37815535068511963
Epoch: 10, Batch: 1523, Loss: 0.5261853933334351
Epoch: 10, Batch: 1524, Loss: 0.37909644842147827
Epoch: 10, Batch: 1525, Loss: 0.4216509461402893
Epoch: 10, Batch: 1526, Loss: 0.3491060137748718
Epoch: 10, Batch: 1527, Loss: 0.2303299605846405
Epoch: 10, Batch: 1528, Loss: 0.3007257282733917
Epoch: 10, Batch: 1529, Loss: 0.38113832473754883
Epoch: 10, Batch: 1530, Loss: 0.14930939674377441
Epoch: 10, Batch: 1531, Loss: 0.393784761428833
Epoch: 10, Batch: 1532, Loss: 0.2610202431678772
Epoch: 10, Batch: 1533, Loss: 0.1811806559562683
Epoch: 10, Batch: 1534, Loss: 0.2526363730430603
Epoch: 10, Batch: 1535, Loss: 0.13582628965377808
Epoch: 10, Batch: 1536, Loss: 0.1567872017621994
Epoch: 10, Batch: 1537, Loss: 0.14692601561546326
Epoch: 10, Batch: 1538, Loss: 0.15637098252773285
Epoch: 10, Batch: 1539, Loss: 0.2436765432357788
Epoch: 10, Batch: 1540, Loss: 0.6884388327598572
Epoch: 10, Batch: 1541, Loss: 0.33192765712738037
Epoch: 10, Batch: 1542, Loss: 0.26023727655410767
Epoch: 10, Batch: 1543, Loss: 0.4845704734325409
Epoch: 10, Batch: 1544, Loss: 0.6520872712135315
Epoch: 10, Batch: 1545, Loss: 0.525136411190033
Epoch: 10, Batch: 1546, Loss: 0.40626826882362366
Epoch: 10, Batch: 1547, Loss: 0.26432734727859497
Epoch: 10, Batch: 1548, Loss: 0.3731198012828827
Epoch: 10, Batch: 1549, Loss: 0.5812228322029114
Epoch: 10, Batch: 1550, Loss: 0.3548191487789154
Epoch Completed: 10/10, Time: 212.83747339248657, Train Loss: 0.3426711357889637, Valid Loss: 0.34558465211622175

TESTING PHASE: creating index...
index created!
Test:  [ 0/40]  eta: 0:00:14  model_time: 0.3233 (0.3233)  evaluator_time: 0.0028 (0.0028)  time: 0.3636  data: 0.0345  max mem: 4996
Test:  [39/40]  eta: 0:00:00  model_time: 0.3253 (0.3212)  evaluator_time: 0.0036 (0.0035)  time: 0.3584  data: 0.0340  max mem: 4996
Test: Total time: 0:00:14 (0.3628 s / it)
Averaged stats: model_time: 0.3253 (0.3212)  evaluator_time: 0.0036 (0.0035)
Accumulating evaluation results...
DONE (t=0.04s).
IoU metric: bbox
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.054
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.045
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.091
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.159
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.100
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.162
